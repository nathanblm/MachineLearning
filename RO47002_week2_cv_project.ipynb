{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RO47002 Machine Learning for Robotics\n",
    "* (c) TU Delft, 2024\n",
    "* Period: 2024-2025, Q1\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/682421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"\"\n",
    "STUDENT_NAME1 = \"\"\n",
    "STUDENT_NUMBER1 = \"\"\n",
    "STUDENT_NAME2 = \"\"\n",
    "STUDENT_NUMBER2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3f76d6a626db81c484191482b101edb",
     "grade": true,
     "grade_id": "cell-c35e4c8223095209",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you and your lab partner alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled practicum hours to ask a TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3c821580205dcfdb19fbda491ae3006",
     "grade": false,
     "grade_id": "cell-6dfe7d328a3fc00c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practicum 2\n",
    "* Topic: data collection & annotation, feature extraction, model exploration, visualization\n",
    "* Before performing this practicum, work through **Book chapter(s): 2, 3**\n",
    "* **Deadline**: Monday, September 16, 2024, 23:59\n",
    "\n",
    "## Objectives\n",
    "In this practicum, you and your lab partner will experiment with data collection, annotation, and turning a simple computer vision task into a machine learning problem.\n",
    "This practicum will not go very deep in machine learning theory, but is more intended to make you familiar with the broader context of how machine learning can be applied in robotics, and experience some practical steps in performing your own data collection.\n",
    "You will also demonstrate that you can perform several basic tasks with sklearn which have been explained in the book chapters.\n",
    "\n",
    "\n",
    "The assignment is roughly organised into the steps of the \"Machine Learning Project Checklist\" explained in *Chapter 2 and Appendix B of the book*:\n",
    "\n",
    "1. [Framing the problem](#framing_the_problem)\n",
    "2. [Get the data](#get_the_data)\n",
    "3. [Explore the data](#explore_the_data)\n",
    "4. [Prepare the data](#prepare_the_data)\n",
    "5. [Explore different models](#explore_different_models)\n",
    "6. [Fine-tune your models](#fine_tune_your_models)\n",
    "7. [Present your solution](#present_your_solution)\n",
    "8. [Launch, monitor, and maintain your system](#maintain_your_system)\n",
    "\n",
    "Of course, there are many more decisions and variations that could be explored at each step, but we will keep it simple for this exercise, so do not take this assignment as an exact template of what should or should not be done in each step. In real life, you would probably also move back and forward between these steps, revising some earlier decisions as you gain more insight on the problem. Nevertheless, this practicum provides a good guideline of how to approach such a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c75fad52b68607eb542f8db1fd2b79a4",
     "grade": false,
     "grade_id": "cell-faaced7dc986d3ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='framing_the_problem'></a>\n",
    "# 1. Framing the problem\n",
    "The context of this practicum is of a robot which will need to be able to pickup a pen from a desk or table. The robot has a basic down-facing camera that it can place on top of the desk to inspect an area of interest.\n",
    "The goal is to detect within this area where the pen is located, and how it is oriented.\n",
    "To train the robot's perception system, we obtain a \"pen image dataset\" where the pen's extents (the physical tip and end) have been manually annotated in each image.\n",
    "When the robot is deployed (a.k.a. *test* time), the robot will have to determine the locations of the tip and end of a pen in front of its camera.\n",
    "\n",
    "How can we design a system that takes an image as input, and outputs the location and orientation of the pen?\n",
    "We might consider two approaches:\n",
    "\n",
    "* **Formulate this as a _regression_ problem:** Given an image, convert the full image to some feature representation and train a regressor that outputs 4 values, namely the (x,y) image locations of the tip, and the (x,y) locations of the end of the pen.\n",
    "\n",
    "* **Formulate this as a _classification_ problem:** Divide the given image into small image patches. For each patch in the image, classifiy if this patch is the tip (start) of the pen, the end of the pen, the middle of the pen, or if it is just background. If we could succesfully classify patches at a diverse set of locations in the image, we would could determine the most probable image locations where the pen starts and ends, and thus its shape and orientation relative to the camera.\n",
    "\n",
    "While the regression formulation might seem more natural, processing many local image regions in parallel is a more common practice in many machine learning for computer vision applications. The classification formulation could for instance be extended to work with images containing multiple pens, which simply ammounts to more image patches containing a tip, end or middle section, so the input and output space of the learned hypothesis function remains the same. In contrast, for regression we would need to change the output space to 8 values (namely four (x,y) coordinates) if we want to deal with two pens, thus changing the structure of the hypothesis function compared to the single pen case. Each different number of pens would thus require a different hypothesis function if we apply regression naively this way.\n",
    "\n",
    "We therefore follow the **classification** approach in this assignment.\n",
    "\n",
    "With this information our robot should have enough information to position and orient its (imaginary) grasper, and pick up the pen and put it in its pen collection.\n",
    "\n",
    "\n",
    "![Robot holding a pen](extra/grasping-robot.jpg)\n",
    "Image source: https://gigazine.net/gsc_news/en/20101221_robo_xero/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "275dc147c20f07c66a4dc904a6419598",
     "grade": false,
     "grade_id": "cell-c6ccf58badf36530",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726344/View\">Lecture 2B - Case Study Visual Object Detection</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "297f7ce61e430821cfb2f4f0d674aa8e",
     "grade": false,
     "grade_id": "cell-b22fb821f8a8960f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Schematic Overview\n",
    "Below you find a schematic overview of the intended way our robot should detect the position and orientation of the pen.\n",
    "![Schematic overview of our approach](extra/schematic-prac2.jpg)\n",
    "\n",
    "The main steps of the operational procedure in this figure are:\n",
    "1. Take the input image, and extract smaller image patches around a diverse set of image location (shown as blue dots).\n",
    "2. Once the patches are extracted, each patch is converted into some feature vector $x$.\n",
    "3. The feature vectors are then fed to a trained classifier, which will assign to a patch one of four class labels:  background, tip of the pen, end of the pen, or middle of the pen.\n",
    "4. After all patches in the image are classified, we post-process the result. Here we select the location of the patch for which the classifier is most confident that it belongs to the pen's tip, and the one for which it is most confident that it belongs to the pen's end. We assume the pen is located along the line connecting these points.\n",
    "\n",
    "To built such a system, we thus first have to create a labelled dataset $X$ of image patches with the four class labels. With this dataset, we can select and optimise a suitable classifier. Other important choices are how many patches of each class we include in the training data, and how we represent each patch as a feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dc85809e46597117067bb796ae02da4",
     "grade": false,
     "grade_id": "cell-345010a783ab5302",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Setup common python stuff\n",
    "We will start by loading a few common python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.445775Z",
     "start_time": "2023-09-07T11:10:53.433695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python â‰¥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, glob\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.455270Z",
     "start_time": "2023-09-07T11:10:53.450566Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.461614Z",
     "start_time": "2023-09-07T11:10:53.458464Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0315a5a5d80180fec7b643c400c7aabc",
     "grade": false,
     "grade_id": "cell-aa9612c6df723c61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='get_the_data'></a>\n",
    "# 2. Get the data\n",
    "\n",
    "For this assignment, you will first work on an annotated data set, consisting of annotated images of pens.\n",
    "Let's make sure we can load the image and annotation data.\n",
    "\n",
    "We start by listing the jpg files in the directory `images/mypen/`.\n",
    "If everything is correct, we should find 37 jpg images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.467616Z",
     "start_time": "2023-09-07T11:10:53.464920Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "def list_images(image_dir, filename_expression='*.jpg'):\n",
    "    # Convert the filename expression to a regex pattern, replacing '*' with '.*' and adding start (^) and end ($) anchors\n",
    "    filename_expression_regex = re.compile('^' + filename_expression.replace('*', '.*') + '$', re.IGNORECASE)\n",
    "    # List all files in the directory\n",
    "    all_files = glob.glob(os.path.join(image_dir, '*'))\n",
    "    # Filter files using the regex pattern to match case-insensitively\n",
    "    filenames = [f for f in all_files if filename_expression_regex.match(os.path.basename(f))]\n",
    "    filenames = sorted(filenames)  # important for cross-platform compatibility\n",
    "    print(f'Found {len(filenames)} image files in the directory \"{image_dir}\"')\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.473033Z",
     "start_time": "2023-09-07T11:10:53.469039Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'images/mypen'\n",
    "\n",
    "# list all images. There should be 37 images in the images/mypen/ directory\n",
    "filenames = list_images(IMAGE_DIR)\n",
    "N = len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b405870cbf7399797a7ec5b6495fcd6c",
     "grade": false,
     "grade_id": "cell-f1bad3b730bcfc42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Le's see if we can load the first image and visualize it to verify that it has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.740589Z",
     "start_time": "2023-09-07T11:10:53.475391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check if we can load and visualize an image here\n",
    "I = plt.imread(filenames[0])\n",
    "\n",
    "# this should show a picture of a pen\n",
    "plt.figure()\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e30d49179274339557faa5fe8073315c",
     "grade": false,
     "grade_id": "cell-e0739afaa29cc360",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To better understand how such image data is represented as a numpy array, let's start simple by creating 3 function to determine the width, height, and number of color channels of a loaded image. Channels would be 3 for a color image (Red, Green, Blue), and for instance 1 for a grayscale image.\n",
    "\n",
    "Note that images are loaded as numpy matrices, and hence pixels are indexed similar as mathematical arrays, i.e. (row, column). This means that the first dimension of the image goes into the vertical direction, and the second dimension into the horizontal direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.744971Z",
     "start_time": "2023-09-07T11:10:53.742094Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94da11041047e6bc30a2512651250895",
     "grade": false,
     "grade_id": "cell-b8a4d9169e0e5919",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_image_width(I):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def get_image_height(I):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def get_image_channels(I):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:53.749612Z",
     "start_time": "2023-09-07T11:10:53.746279Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea255154b9db2960e281df182614dbf1",
     "grade": true,
     "grade_id": "cell-9694201947b184c3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(get_image_width(I) == 1024)\n",
    "assert(get_image_height(I) == 768)\n",
    "assert(get_image_channels(I) == 3)\n",
    "\n",
    "I2 = np.swapaxes(I, 0, 1) # swap the width and height, but keep the 3rd (color channel) dimension in place\n",
    "assert(get_image_width(I2) == 768)\n",
    "assert(get_image_height(I2) == 1024)\n",
    "assert(get_image_channels(I2) == 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c9c26df3d16e6f294a6e71162667a34",
     "grade": false,
     "grade_id": "cell-d5d0cb8250fc679a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Load all images and annotations\n",
    "\n",
    "Previously, we found all images in the given directory. Now we are going to load all of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.105056Z",
     "start_time": "2023-09-07T11:10:53.751030Z"
    }
   },
   "outputs": [],
   "source": [
    "Is = [plt.imread(filename) for filename in filenames]\n",
    "print('loaded %d images' % len(Is))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1057e7153a3e2b13e2cf1c46da792550",
     "grade": false,
     "grade_id": "cell-52c0d5f51edc9d0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The data has been annotated by a human labeler manually clicking twice in each on image, such that the image coordinates of the first click indicate the location of the tip of the pen, and the second click of the end of the pen.\n",
    "We can imagine a line between these locations, running along the length of the pen, such that image locations near the middle part of the line correspond to the center of the pen.\n",
    "\n",
    "The annotations are therefore given as a $37 \\times 4$ a matrix, where each $i$-th row contains the annotations of the $i-th$ image. A row contains for values, $(x_\\textrm{tip},y_\\textrm{tip},x_\\textrm{end},y_\\textrm{end})$, the image coordinates of the tip and end of the pen.\n",
    "\n",
    "Using the standard python `pickle` or `numpy.load` module, load the data annotations which are stored as a numpy array in `annots.npy` which is stored in the same directory as the images. You can use the built-in function in `pickle` (doc: https://docs.python.org/3/library/pickle.html) or `numpy.load` function (doc: https://numpy.org/doc/stable/reference/generated/numpy.load.html).\n",
    "If everything is correct, you will end up with a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.110481Z",
     "start_time": "2023-09-07T11:10:54.107732Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db9a455cc48fb14a42582e3bd9fcf2ec",
     "grade": false,
     "grade_id": "cell-9229c5719421dbc8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "annots = None # store your solution in this variable name\n",
    "annot_filename = os.path.join(IMAGE_DIR, 'annots.npy')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.115962Z",
     "start_time": "2023-09-07T11:10:54.112207Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8247a3deec521e1ff787f7e07873593d",
     "grade": true,
     "grade_id": "cell-60b7105eea9e7370",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# if the annotations were correctly loaded, they should be a N x 4 numpy array\n",
    "assert(annots is not None)\n",
    "assert(type(annots) == np.ndarray)\n",
    "assert(annots.shape == (37, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ce7ebf1eeb25b59a4db3c96e7b8cc1",
     "grade": false,
     "grade_id": "cell-b426f6802da30467",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Split the data into a train and test set\n",
    "\n",
    "If everything is correct, you should now have loaded 37 images and their annotations. For the remainder of this exercises, we will now fix which labeled image data we shall consider as training data, and which for testing. We do this by simply storing the image indices of each split in a variable that we can use throughout the exercise.\n",
    "\n",
    "- training data will consist of the first 26 images\n",
    "- test data consists of the remaining 11 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.120999Z",
     "start_time": "2023-09-07T11:10:54.117556Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imgs = list(range(0,26))\n",
    "test_imgs = list(range(26,len(Is)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bae0512fb9c3b5a97e204176e8d1f336",
     "grade": false,
     "grade_id": "cell-91a0b3ce2d144155",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='explore_the_data'></a>\n",
    "# 3. Explore the data\n",
    "\n",
    "Once we sucessfully loaded all images and annotations, we are now ready to implement a function to plot them. We can use `matplotlib.pyplot` (https://matplotlib.org/3.3.1/api/pyplot_summary.html) for this, which provides a very similar interface to the plotting commands found in Matlab.\n",
    "\n",
    "In this plotting routine, you should execute the following steps:\n",
    "\n",
    "1. show the image with `plt.imshow`, plot the following steps on top of the image\n",
    "1. plot point p1 as a green circle with `plt.plot`, with markersize 10, and label \"tip\"\n",
    "1. plot point p2 as a blue circle, with markersize 10, and label \"end\"\n",
    "1. plot a yellow line starting at one point and ending at the other, give it linewdith 2 for more clarity\n",
    "1. Add a legend with `plt.legend` (pro-tip: if you use the \"label\" keyword to the plot() commands to [simplify creating the legend](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.legend.html))\n",
    "\n",
    "Use the matplotlib documentation to find basic usage information on these plotting routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.126104Z",
     "start_time": "2023-09-07T11:10:54.122786Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "508a75c19a934b8a18431637be4915ba",
     "grade": true,
     "grade_id": "cell-01144d0a52bac1d6",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def show_annotation(I, p1, p2):\n",
    "    plt.figure()\n",
    "    \n",
    "    # show the image\n",
    "    # plot point p1 as a green circle, with markersize 10, and label \"tip\"\n",
    "    # plot point p2 as a blue circle, with markersize 10, and label \"end\"\n",
    "    # plot a yellow line starting at one point and ending at the other, give it linewdith 2 for more clarity\n",
    "    # Use a suitable color and linewidth for better visualization\n",
    "    # Add a legend (tip, you can use the \"label\" keyword when you plot a point)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # done, show the image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, this call to `show_annotation()` below should show a similar image as this reference:\n",
    "![reference to first image](extra/reference_annotation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.377467Z",
     "start_time": "2023-09-07T11:10:54.127279Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7fdaff88a2fb54a0e59fc38aae1c49b",
     "grade": false,
     "grade_id": "cell-d2c44842763a2d81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "I = Is[img_idx]\n",
    "p1 = annots[img_idx,:2].copy() # point 1, tip of the pen\n",
    "p2 = annots[img_idx,2:].copy() # point 2, end of the pen\n",
    "\n",
    "show_annotation(I, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0d2302b8e51a25a98621e3121f14fba",
     "grade": false,
     "grade_id": "cell-db3260c632fae262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using jupyter's interactive widgets, we can now make a simple data explorer using a slider.\n",
    "\n",
    "Using the powerful [`ipywidgets.interact`](https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Basic-interact),\n",
    "we can make a simple interactive widgets in a jupyter notebook to aid data exploration.\n",
    "Note that every time we move the slide, `interact` we execute the plotting function that we gave it again, refreshing the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.640666Z",
     "start_time": "2023-09-07T11:10:54.378789Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad59f8b66c04b8100cec8888997c841d",
     "grade": false,
     "grade_id": "cell-60fda59c9df0d9c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def show_annotated_image(j, train_imgs, Is, annots):\n",
    "    # Show the j-th training image\n",
    "    img_idx = train_imgs[j]\n",
    "    \n",
    "    I = Is[img_idx]\n",
    "    p1 = annots[img_idx, :2].copy()  # point 1, tip of the pen\n",
    "    p2 = annots[img_idx, 2:].copy()  # point 2, end of the pen\n",
    "\n",
    "    show_annotation(I, p1, p2)\n",
    "\n",
    "ipywidgets.interact(\n",
    "    show_annotated_image, \n",
    "    j=(0, len(train_imgs) - 1), \n",
    "    train_imgs=ipywidgets.fixed(train_imgs), \n",
    "    Is=ipywidgets.fixed(Is), \n",
    "    annots=ipywidgets.fixed(annots)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this stage to study your problem, and reflect on what types of variations are present (e.g. pen orientation?) or missing (e.g. different backgrounds?). You can also anticipate if there are any possible causes for noise or label errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e4640bfe8e2d61e78707c4d27146883",
     "grade": false,
     "grade_id": "cell-35072fae5b2dab02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='prepare_the_data'></a>\n",
    "# 4. Prepare the data\n",
    "\n",
    "At this stage, we turn our images into a set $X$ of feature vectors with labels $y$ that can be used to train classifiers.\n",
    "Remember that we somehow need to address the following issues here:\n",
    "* How to select a useful and diverse set of patches from the images\n",
    "* How to turn each image patch into a feature vector\n",
    "* How to determine the class label for each patch, given we currently only have two annotated image locations (tip and end of pen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c868dc6591f712cb7603961080ce62a",
     "grade": false,
     "grade_id": "cell-f1094c44127c8138",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1 Sample locations in image to extract patches\n",
    "\n",
    "From each image we will extract multiple image patches, where each patch will be a small subregion of the image of $100 \\times 100$ pixels.\n",
    "\n",
    "Some of the patches will contain background, and others will contain parts of the pen.\n",
    "Since most regions in the image contain background (the pen occupies only a relatively small part of the image), there is a risk that patches containing only background will be overrepresented in the training data. Such as *class inbalance* might result in our future classifier becoming *biased* to predict too many cases as background without considering the other classes, as that would result in a good training score.\n",
    "\n",
    "We therefore will consider two strategies to sample patch locations from an image:\n",
    "\n",
    "1. sample in a uniform grid across the image, which will have a lot of background patches,\n",
    "2. sample some points similar to strategy 1, but also select additional points only around the pen\n",
    "\n",
    "We'll also add some random offsets to the pixel coordinates of the sampled locations to add some more variance to the data,\n",
    "and avoid accidentally having the exact same image location multiple times.\n",
    "\n",
    "The code to implement these sampling strategies has already been given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.644336Z",
     "start_time": "2023-09-07T11:10:54.641926Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb6822b06af9312b03a5cb03c1b9d3d",
     "grade": false,
     "grade_id": "cell-73e36739d6f7c1a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the size of the patch in pixels\n",
    "WIN_SIZE = (100, 100, 3)\n",
    "\n",
    "# for convenience, half the window\n",
    "HALF_WIN_SIZE = (WIN_SIZE[0] // 2, WIN_SIZE[1] // 2, WIN_SIZE[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:54.652421Z",
     "start_time": "2023-09-07T11:10:54.645494Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4aedb76a3821a4f341d71f6d45c8bf38",
     "grade": false,
     "grade_id": "cell-31ef718910808767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_points_grid(I):\n",
    "    # window centers\n",
    "    W = get_image_width(I)\n",
    "    H = get_image_height(I)\n",
    "    \n",
    "    step_size = (WIN_SIZE[0]//2, WIN_SIZE[1]//2)\n",
    "    min_ys = range(0, H-WIN_SIZE[0]+1, step_size[0])\n",
    "    min_xs = range(0, W-WIN_SIZE[1]+1, step_size[1])\n",
    "    center_ys = range(HALF_WIN_SIZE[0], H-HALF_WIN_SIZE[0]+1, step_size[0])\n",
    "    center_xs = range(HALF_WIN_SIZE[1], W-HALF_WIN_SIZE[1]+1, step_size[1])\n",
    "    centers = np.array(np.meshgrid(center_xs, center_ys))\n",
    "    centers = centers.reshape(2,-1).T\n",
    "    centers = centers.astype(float) \n",
    "    \n",
    "    # add a bit of random offset\n",
    "    centers += np.random.rand(*centers.shape) * 10 \n",
    "    \n",
    "    # discard points close to border where we can't extract patches\n",
    "    centers = remove_points_near_border(I, centers)\n",
    "    \n",
    "    return centers\n",
    "\n",
    "def sample_points_around_pen(I, p1, p2):\n",
    "    Nu = 100 # uniform samples (will mostly be background, and some non-background)\n",
    "    Nt = 50 # samples at target locations, i.e. near start, end, and middle of pen\n",
    "    \n",
    "    target_std_dev = np.array(HALF_WIN_SIZE[:2])/3 # variance to add to locations\n",
    "\n",
    "    upoints = sample_points_grid(I)\n",
    "    idxs = np.random.choice(upoints.shape[0], Nu)\n",
    "    upoints = upoints[idxs,:]\n",
    "    \n",
    "    \n",
    "    # sample around target locations\n",
    "    tpoints1 = np.random.randn(Nt,2)\n",
    "    tpoints1 = tpoints1 * target_std_dev + p1\n",
    "\n",
    "    tpoints2 = np.random.randn(Nt,2)\n",
    "    tpoints2 = tpoints2 * target_std_dev + p2\n",
    "\n",
    "    # sample over length pen\n",
    "    alpha = np.random.rand(Nt)\n",
    "    tpoints3 = p1[None,:] * alpha[:,None] + p2[None,:] * (1. - alpha[:,None])\n",
    "    tpoints3 = tpoints3 + np.random.randn(Nt,2) * target_std_dev\n",
    "    \n",
    "    # merge all points\n",
    "    points = np.vstack((upoints, tpoints1, tpoints2, tpoints3))\n",
    "    \n",
    "    # discard points close to border where we can't extract patches\n",
    "    points = remove_points_near_border(I, points)\n",
    "    \n",
    "    return points\n",
    "\n",
    "def remove_points_near_border(I, points):\n",
    "    W = get_image_width(I)\n",
    "    H = get_image_height(I)\n",
    "\n",
    "    # discard points that are too close to border\n",
    "    points = points[points[:,0] > HALF_WIN_SIZE[1],:]\n",
    "    points = points[points[:,1] > HALF_WIN_SIZE[0],:]\n",
    "    points = points[points[:,0] < W - HALF_WIN_SIZE[1],:]\n",
    "    points = points[points[:,1] < H - HALF_WIN_SIZE[0],:]\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to understand the difference between these two sampling strategies is to visualize the locations of the center of the patches that they generate. In the figures below, each white dot indicates the center location of a patch.\n",
    "You can see that strategy 2 would generate many more pathes containing parts of the pen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.081219Z",
     "start_time": "2023-09-07T11:10:54.653629Z"
    }
   },
   "outputs": [],
   "source": [
    "points1 = sample_points_grid(I) # sampling strategy 1\n",
    "points2 = sample_points_around_pen(I, p1, p2) # sampling strategy 2\n",
    "\n",
    "# plot both sampling strategies in a single figure using subplots\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(I)\n",
    "plt.plot(points1[:,0], points1[:,1], 'w.')\n",
    "plt.title('sampling strategy 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(I)\n",
    "plt.plot(points2[:,0], points2[:,1], 'w.')\n",
    "plt.title('sampling strategy 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db3230161c59fb7aec31275180bdd4f",
     "grade": false,
     "grade_id": "cell-93a3e6a0d50390ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2 Extract patches at a target location\n",
    "\n",
    "You will now write a hand-crafted feature extraction, a simple function of a few lines of code which:\n",
    "\n",
    "- takes as first input `I`: the RGB input image \n",
    "- takes as second input `p`: a 2D location in pixel coordinates\n",
    "- returns the $100 \\times 100 \\times 3$ image patch centered around pixel coordinates p\n",
    "\n",
    "To extract an image patch, you can use array slicing as you learned in week 1.\n",
    "\n",
    "Note that point p will be given as a 2-dimensional floating point numpy vector. You should cast the elements to `int` types before you can use them to slice an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.085558Z",
     "start_time": "2023-09-07T11:10:55.082555Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "682abab9e1fb9cd2fc622bdac81fb4dd",
     "grade": false,
     "grade_id": "cell-66851531c72ea489",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_patch_at_point(I, p):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution, let's extract the patch around the annotated tip of the pen. If everything is correct, you should be able to see the tip similar to this reference image: ![reference patch](extra/reference_patch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.186908Z",
     "start_time": "2023-09-07T11:10:55.086755Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7364cf4637b36f60e274509ea1534330",
     "grade": true,
     "grade_id": "cell-1680c0fa41e66dbd",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "P = get_patch_at_point(I, p1)\n",
    "plt.imshow(P)\n",
    "plt.show()\n",
    "\n",
    "assert(get_image_width(P) == 100)\n",
    "assert(get_image_height(P) == 100)\n",
    "assert(get_image_channels(P) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Determine the label of a patch\n",
    "\n",
    "To determine the labels of a patch,\n",
    "we'll consider the distance of the patch's center to the tip of the pen (class 1),\n",
    "to the end of the pen (class 2), or to the middle of the pen (class 3).\n",
    "We will assign the class label based on which distance is the shortest,\n",
    "but only if this shortest distance is under a certain threshold since the pen should still be visible within the patch.\n",
    "If the patch is too far away from the pen, we will mark it as background (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.197119Z",
     "start_time": "2023-09-07T11:10:55.189152Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    'background', # class 0\n",
    "    'tip',        # class 1\n",
    "    'end',        # class 2\n",
    "    'middle'      # class 3\n",
    "]\n",
    "\n",
    "def make_labels_for_points(I, p1, p2, points):\n",
    "    \"\"\" Determine the class label (as an integer) on point distance to different parts of the pen \"\"\"\n",
    "    num_points = points.shape[0]\n",
    "    \n",
    "    # for all points ....\n",
    "    \n",
    "    # ... determine their distance to tip of the pen\n",
    "    dist1 = points - p1\n",
    "    dist1 = np.sqrt(np.sum(dist1 * dist1, axis=1))\n",
    "    \n",
    "    # ... determine their distance to end of the pen\n",
    "    dist2 = points - p2\n",
    "    dist2 = np.sqrt(np.sum(dist2 * dist2, axis=1))\n",
    "\n",
    "    # ... determine distance to pen middle\n",
    "    alpha = np.linspace(0.2, 0.8, 100)\n",
    "    midpoints = p1[None,:] * alpha[:,None] + p2[None,:] * (1. - alpha[:,None]) \n",
    "    dist3 = scipy.spatial.distance_matrix(midpoints, points)\n",
    "    dist3 = np.min(dist3, axis=0)\n",
    "    \n",
    "    # the class label of a point will be determined by which distance is smallest\n",
    "    #    and if that distance is at least below `dist_thresh`, otherwise it is background\n",
    "    dist_thresh = WIN_SIZE[0] * 2./3.\n",
    "\n",
    "    # store distance to closest point in each class in columns\n",
    "    class_dist = np.zeros((num_points, 4))\n",
    "    class_dist[:,0] = dist_thresh\n",
    "    class_dist[:,1] = dist1\n",
    "    class_dist[:,2] = dist2\n",
    "    class_dist[:,3] = dist3\n",
    "    \n",
    "    # the class label is now the column with the lowest number\n",
    "    labels = np.argmin(class_dist, axis=1)\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.683865Z",
     "start_time": "2023-09-07T11:10:55.198680Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_labeled_points(points, labels):\n",
    "    plt.plot(points[labels == 0, 0], points[labels == 0, 1], 'r.', label=CLASS_NAMES[0])\n",
    "    plt.plot(points[labels == 1, 0], points[labels == 1, 1], 'g.', label=CLASS_NAMES[1])\n",
    "    plt.plot(points[labels == 2, 0], points[labels == 2, 1], 'b.', label=CLASS_NAMES[2])\n",
    "    plt.plot(points[labels == 3, 0], points[labels == 3, 1], 'y.', label=CLASS_NAMES[3])\n",
    "\n",
    "labels1 = make_labels_for_points(I, p1, p2, points1)\n",
    "labels2 = make_labels_for_points(I, p1, p2, points2)\n",
    "\n",
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(I)\n",
    "plot_labeled_points(points1, labels1)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(I)\n",
    "plot_labeled_points(points2, labels2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the images, the sampling strategies result in different amount of patches being extracted. In the first strategy, for instance, there are indeed many more \"background\" patches than in the second one.\n",
    "\n",
    "Instead of only looking at images to get a \"qualitative\" feel of the data distribution, it would be good to also be able to quantify how (im)balanced these class distributions are.\n",
    "We should therefore first determine the number of samples of each class in `labels1` and `labels2`.\n",
    "\n",
    "Complete the function `count_classes(labels)` below, which for a given array of `labels` creates a 4-dimensional numpy vector, where the i-th element contains the number of times that class occurs in `labels`. We then use this to count each class occurs in the labels obtained with each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.688418Z",
     "start_time": "2023-09-07T11:10:55.685460Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82f5954f364455c44980d998278ed4bc",
     "grade": false,
     "grade_id": "cell-bb459267b8ad750c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_classes(labels):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.694830Z",
     "start_time": "2023-09-07T11:10:55.689923Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17418bc1c4a428165efd74a68e834ee4",
     "grade": true,
     "grade_id": "cell-44670b8d33ec52b9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(np.all(count_classes([3,0,2,3,1,0]) == [2,1,1,2])) # 2 zeros, 1 one, 1 two, 2 threes\n",
    "assert(np.all(count_classes([3,2,1,2]) == [0,1,2,1])) # 0 zeros, 1 one, 2 twos, 1 three\n",
    "\n",
    "# dtype of resulting array should be integer\n",
    "cnt = count_classes([3,2,1,2])\n",
    "assert(np.issubdtype(cnt.dtype, np.integer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.703895Z",
     "start_time": "2023-09-07T11:10:55.698187Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a119c8f8a6efcfbe592fed8978c6cdcd",
     "grade": false,
     "grade_id": "cell-38d343abdda7c2c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use your count_classes function to determine how often each class labels occurs when using each strategy\n",
    "class_counts1 = count_classes(labels1)\n",
    "class_counts2 = count_classes(labels2)\n",
    "print('class occurrences with strategy 1:', class_counts1)\n",
    "print('class occurrences with strategy 2:', class_counts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from these numbers that in strategy 1 one class is very frequent, and the others much less so. With strategy 2, the classes are more uniformly distributed.\n",
    "\n",
    "As an exercise, we can try to express the amount of 'uniformity' in a distribution using a key concept from information theory: *entropy*. This concept is also related to the concept of *cross-entropy* which you will learn about more later in the course. On YouTube you can find [a good video introduction on entropy and cross-entropy](https://www.youtube.com/watch?v=ErfnhcEV1O8).\n",
    "For now, it suffices to state that entropy measures the amount of 'surprise' or 'uncertainty' we would have about the outcome if we would sample from a given distribution. For instance, if the distribution over the four classes would be so skewed that all samples belong to one class, the entropy would be 0 as there would be no surprise what class label we would see if we take a random sample.\n",
    "On the other hand, the maximum entropy is achieved when all classes are completely equally likely to occur, i.e. the class labels would be uniformly distributed.\n",
    "\n",
    "So, to compute the entropy of the class label counts of the two sampling strategies, first implement the following two functions:\n",
    "1. a function `class_probs(counts)` which takes the class occurence `counts`, and return a distribution vector $\\boldsymbol{p} = [p_1, p_2, p_3, p_4]$ , i.e. a vector of the same length (number of classes), with elements $p_c \\in [0,1]$ which sum up to 1, $\\sum_c p_c = 1$. Each element $p_c$ represent the class probability $P(c)$ that a sample in the distribution has class label $c$.\n",
    "2. an `entropy(p)` function which takes a class distribution vector `p`, and computes the entropy for that distribution. The formula will be given below.\n",
    "\n",
    "Afterwards, you can compute the entropy of each strategy,\n",
    "and compared to the theoretic maximum entropy for a perfectly uniform distribution over the four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.710109Z",
     "start_time": "2023-09-07T11:10:55.706683Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08164a8068e285efb7be6ae4d61622ef",
     "grade": false,
     "grade_id": "cell-d831c1b0b71a897f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def class_probs(counts):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.719747Z",
     "start_time": "2023-09-07T11:10:55.712270Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6452767fab0cb2bf906886c89c96c01f",
     "grade": true,
     "grade_id": "cell-96aa8af031dfe0ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check uniform class distribution\n",
    "test_dist_1 = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "assert(np.all(class_probs(np.array([1,1,1,1])) == test_dist_1))\n",
    "assert(np.all(class_probs(np.array([100,100,100,100])) == test_dist_1))\n",
    "\n",
    "# check non-uniform distributions\n",
    "test_dist_2 = np.array([0., 1., 0., 0.])\n",
    "assert(np.all(class_probs(np.array([0,42,0,0])) == test_dist_2))\n",
    "\n",
    "test_dist_3 = np.array([0., 0.5, 0., 0.5])\n",
    "assert(np.all(class_probs(np.array([0,1,0,1])) == test_dist_3))\n",
    "\n",
    "# should sum up to one for any given of class counts\n",
    "assert(np.sum(class_probs(np.array([36,20,9,412]))) == 1)\n",
    "assert(np.all(class_probs(np.array([36,20,9,412]))) > 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the [*entropy*](https://en.wikipedia.org/wiki/Entropy_(information_theory)) function:\n",
    "\n",
    "$H = - \\sum_c P(c) log_2(P(c)) $.\n",
    "\n",
    "You can find this formula in many fields of science, such as physics, and different uses tend to use this equation with a different base for the logarithm (e.g. an obvious choice would be the natural logarithm with base $e$). We will here use Shannon's entropy from information theory using base 2. The entropy $H$ can then be interpreted as the expected number of bits of information that is needed to communicate the class label $c$ of a random sample from $P$. Again as an extreme, if all samples would have the same class label, there would be no \"uncertainty\", and no information needs to be communicated about the label of an arbitrary sample, corresponding to $H = 0$ bits of information. On the other hand, if you have 8 equally probable class labels and thus full uncertainty about the label of any random sample, you would be expected to communicate $H = 3$ bits of information to indicate which of the $2^3 = 8$ labels that particular sample has.\n",
    "\n",
    "Note that this formula assumes that all classes have at least a non-zero chance of occurring, and you might run into numeric issues if $P(c) = 0$ for one or more classes $c$. The best thing to do is to remove any zero-probability classes if they occur.\n",
    "\n",
    "For this exercise, you must implement this function yourself. You can numpy functions, but not any other statistical python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.727443Z",
     "start_time": "2023-09-07T11:10:55.725027Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc4f12fd300e39c6d1e36802664c575",
     "grade": false,
     "grade_id": "cell-89c7dc098110ebff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.732236Z",
     "start_time": "2023-09-07T11:10:55.728822Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46c07de34a8cf15fec29d9108356737",
     "grade": true,
     "grade_id": "cell-f263f87aced77223",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1-class test case\n",
    "#  (only one class distribution possible, no uncertainty on outcome)\n",
    "assert(entropy(np.array([1.0])) == 0)\n",
    "\n",
    "# 2-class cases\n",
    "# For 2 classes, the entropy should always be between 0.0 and 1.0\n",
    "assert(entropy(np.array([0.5, 0.5])) == 1.0) # a 50%-50% distribution requires 1 bit of information\n",
    "assert(entropy(np.array([1.0, 0.0])) == 0.0) # a 100%-0% distribution requires 0 bits of information (no uncertainty)\n",
    "# NOTE: if this test above doesn't work,\n",
    "# you may still need to remove the elements with probability 0 from the input vector p\n",
    "\n",
    "# 3-class test cases\n",
    "# entropy of [0.1, 0.6, 0.3] is approx 1.2954618, slightly more than one bit of information\n",
    "assert(np.abs(entropy(np.array([0.1, 0.6, 0.3])) - 1.2954618)<1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.736755Z",
     "start_time": "2023-09-07T11:10:55.733616Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca537e450e9b2a691c9fcebb2d0cd7c6",
     "grade": false,
     "grade_id": "cell-0789c91d2718e943",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ANSWER_STRATEGY1_ENTROPY = None # store your solution in this variable name\n",
    "ANSWER_STRATEGY2_ENTROPY = None # store your solution in this variable name\n",
    "ANSWER_MAX_FOUR_CLASS_ENTROPY = None # store your solution in this variable name\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.748962Z",
     "start_time": "2023-09-07T11:10:55.739487Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1b502159600a4932ba18df64f23fdcb",
     "grade": true,
     "grade_id": "cell-3cf6aeaacc57fb90",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(ANSWER_STRATEGY1_ENTROPY is not None) # check to ensure the right variable name is used\n",
    "assert(ANSWER_STRATEGY2_ENTROPY is not None) # check to ensure the right variable name is used\n",
    "assert(ANSWER_MAX_FOUR_CLASS_ENTROPY is not None) # check to ensure the right variable name is used\n",
    "\n",
    "print('Your computed results:')\n",
    "print('          Entropy for labels in strategy 1:', ANSWER_STRATEGY1_ENTROPY)\n",
    "print('          Entropy for labels in strategy 2:', ANSWER_STRATEGY2_ENTROPY)\n",
    "print('Max. entropy for a four class distribution:', ANSWER_MAX_FOUR_CLASS_ENTROPY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, the results you computed should confirm that the labels obtained with strategy 2 has an entropy close to the theoretic maximum, and thus is in an objective quantifiable sense more uniformly distribution than the class labels obtained with strategy 1.\n",
    "\n",
    "We will revisit entropy in future lectures and (Book) exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6a87a272ed3fdfdde88aac5dffad14f",
     "grade": false,
     "grade_id": "cell-616aea3b289ef53b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.4 Transform patches into feature vectors\n",
    "\n",
    "Now that we can create a dataset of labeled image patches (using either strategy 1 or 2), we focus on *feature extraction*, i.e. the transformation that converts each input patch into a fixed size feature vector for the classifier.\n",
    "A good feature extractor ensures that the important variance in the data is kept and makes the classes easily separable. At the same time, it should also suppress unimportant variations and noise in the data, especially if the amount of training data is limited, since then the classifier will not have to *learn* that such variations should not affect the outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af7cd1155cb43476f7bb27f4c4509b5b",
     "grade": false,
     "grade_id": "cell-46a191275919bafa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726346/View\">Lecture 2C - ML Formalized (part2)</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f29f363029e33a04d89587ea04624ca",
     "grade": false,
     "grade_id": "cell-98f1c49a19d6421c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here you will implement a very simple strategy as the function `patch_to_vec`, which takes an image patch as input, and should return its feature representation, a vector $x$.\n",
    "The function should do the following:\n",
    "\n",
    "1. Resize the patch to a $9 \\times 9 \\times 3$ color image. To resize the image (patch), `skimage.transform` might contain a useful function, check the documentation here: https://scikit-image.org/docs/dev/api/skimage.transform.html. Note that we do not want to use any anti_aliasing features, since these are slow and we need to convert many patches.\n",
    "\n",
    "2. After downsizing the patch, reshape or flatten the patch (which is a 3-dimensional [tensor](https://nl.wikipedia.org/wiki/Tensor)) to a vector (i.e. a 1-dimensional tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.755909Z",
     "start_time": "2023-09-07T11:10:55.751936Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81c499864e6d5359141b6bcef1e5e937",
     "grade": false,
     "grade_id": "cell-0cf0f94cd3a5d5a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the target size of the patches after downsizing\n",
    "FEAT_SIZE = (9,9,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.762684Z",
     "start_time": "2023-09-07T11:10:55.758746Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08d5809eadacdf362575a61dffc0478c",
     "grade": false,
     "grade_id": "cell-7fd750828ba3a31f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def patch_to_vec(P):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How many dimensions will the resulting feature space have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.769869Z",
     "start_time": "2023-09-07T11:10:55.765500Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec270e4866f1a454c947bcc3d52f2d3",
     "grade": false,
     "grade_id": "cell-ebffac303eaefbee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "number_of_feature_dimensions = None # store your solution in this variable name\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert(number_of_feature_dimensions is not None) # check to ensure the right variable name is used\n",
    "\n",
    "print(f'This will be a {number_of_feature_dimensions}-dimensional feature space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.775811Z",
     "start_time": "2023-09-07T11:10:55.772342Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a4da6f010d55b94adb55b18726c218d",
     "grade": true,
     "grade_id": "cell-a14eb6f5c755ea94",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert number_of_feature_dimensions is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Let's put it all together\n",
    "\n",
    "Finally, we put all the preceding steps together to construct our set of feature vectors from a given raw dataset of images. The following code performs the following steps:\n",
    "\n",
    "* For each image in our dataset, do the following\n",
    "    1. select patch locations, using one of the two strategies\n",
    "    2. determine the class label for each location, considering the image annotations\n",
    "    3. extract the image patches at the selected locations\n",
    "    4. convert each image patches to a feature vector\n",
    "* Concatenate all features and labels from the images together in one data matrix `X`, and one large label vector\n",
    "* Also, for each feature vector we keep track in `imgids` from which image it was extracted, and in `points` at which pixel coordinate the patch was located. This will help us later to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:55.785568Z",
     "start_time": "2023-09-07T11:10:55.778628Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_patches(I, p1, p2, strategy=None):\n",
    "    \n",
    "    # by default, if no strategy is explicitly defined, use strategy 2\n",
    "    if strategy == 1:\n",
    "        points = sample_points_grid(I)\n",
    "    if strategy == 2 or strategy is None:\n",
    "        points = sample_points_around_pen(I, p1, p2)\n",
    "    \n",
    "    # determine the labels of the points\n",
    "    labels = make_labels_for_points(I, p1, p2, points)\n",
    "    \n",
    "    xs = []\n",
    "    for p in points:\n",
    "        P = get_patch_at_point(I, p)\n",
    "        x = patch_to_vec(P)\n",
    "        xs.append(x)\n",
    "    X = np.array(xs)\n",
    "\n",
    "    return X, labels, points\n",
    "\n",
    "def extract_multiple_images(Is, idxs, annots, strategy=None):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    points = []\n",
    "    imgids = []\n",
    "\n",
    "    for step, idx in enumerate(idxs):\n",
    "        I = Is[idx]\n",
    "        I_X, I_y, I_points = extract_patches(I, annots[idx,:2], annots[idx,2:], strategy=strategy)\n",
    "\n",
    "        classcounts = count_classes(I_y)\n",
    "        print(f'image {idx}, class count = {classcounts}')\n",
    "\n",
    "        Xs.append(I_X)\n",
    "        ys.append(I_y)\n",
    "        points.append(I_points)\n",
    "        imgids.append(np.ones(len(I_y),dtype=int)*idx)\n",
    "\n",
    "    Xs = np.vstack(Xs)\n",
    "    ys = np.hstack(ys)\n",
    "    points = np.vstack(points)\n",
    "    imgids = np.hstack(imgids)\n",
    "    \n",
    "    return Xs, ys, points, imgids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:57.011769Z",
     "start_time": "2023-09-07T11:10:55.789075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, points_train, imgids_train = extract_multiple_images(Is, train_imgs, annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:57.538332Z",
     "start_time": "2023-09-07T11:10:57.013916Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test, points_test, imgids_test = extract_multiple_images(Is, test_imgs, annots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add a simple interactive ipython widget to quickly inspect the images, and the sampled locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:57.836190Z",
     "start_time": "2023-09-07T11:10:57.541139Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_gt_labels(idx, Is, y_train, points_train, imgids_train):\n",
    "    I = Is[idx]\n",
    "    \n",
    "    I_points = points_train[imgids_train == idx,:]\n",
    "    I_ys = y_train[imgids_train == idx]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(I)\n",
    "    plot_labeled_points(I_points, I_ys)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "ipywidgets.interact(show_gt_labels, \n",
    "                    idx=(0,len(train_imgs)-1), \n",
    "                    Is = ipywidgets.fixed(Is), \n",
    "                    y_train = ipywidgets.fixed(y_train), \n",
    "                    points_train = ipywidgets.fixed(points_train), \n",
    "                    imgids_train = ipywidgets.fixed(imgids_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the samples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:58.662814Z",
     "start_time": "2023-09-07T11:10:57.839728Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_samples(Ps, labels):\n",
    "    uls = np.unique(labels)\n",
    "    nclasses = len(uls)\n",
    "    nsamples = 12\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    for lidx, label in enumerate(uls):\n",
    "        idxs = np.where(labels == label)[0]\n",
    "        idxs = np.random.choice(idxs, nsamples, replace=False)\n",
    "        \n",
    "        for j, idx in enumerate(idxs):\n",
    "            P = Ps[idx,:]\n",
    "            P = P.reshape(FEAT_SIZE)\n",
    "            \n",
    "            plt.subplot(nclasses, nsamples, lidx*nsamples+j+1)\n",
    "            plt.imshow(P, clim=(0,1))\n",
    "            plt.axis('off')\n",
    "            plt.title('label: %d' % label)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "plot_samples(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "652609e9e105399b4a0b389df077a070",
     "grade": false,
     "grade_id": "cell-bdfcccec57580b4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='explore_different_models'></a>\n",
    "# 5. Explore Different Models\n",
    "\n",
    "We can now select some classifiers that we might want to test on this dataset.\n",
    "\n",
    "* a *Logistic Regression classsifier* using the `SGDClassifier` class in the `linear_model` package of sklearn. See also Chapter 3 of the book. Call the classifier object `sgd_clf`. Look into the documentation of `SGDClassifier` to see how to set the loss to the logistic regression loss.\n",
    "* a *Decision Tree* using `DecisionTreeClassifier`. Note that Chapter 2 used the related `DecisionTreeRegressor`, which uses a similar approach, but then for regression\n",
    "* a *Random Forest* using `RandomForestClassifier`. See also Chapter 2.\n",
    "\n",
    "You may need to import the relevant modules from `sklearn`.\n",
    "\n",
    "PS.: Do NOT train the classifiers yet, we'll do that in a notebook cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:58.708222Z",
     "start_time": "2023-09-07T11:10:58.664127Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b87c520be20440d1df88b5a5e626ce54",
     "grade": false,
     "grade_id": "cell-d9b0bcd8c61fda5d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = None # store your solution in variable name\n",
    "dt_clf = None # store your solution in variable name\n",
    "rf_clf = None # store your solution in variable name\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:10:58.712751Z",
     "start_time": "2023-09-07T11:10:58.709629Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94121a63a47290385534683d46016959",
     "grade": true,
     "grade_id": "cell-7f5fbdaa5a4166a8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(sgd_clf is not None) # check to ensure the right variable name is used\n",
    "assert(dt_clf is not None) # check to ensure the right variable name is used\n",
    "assert(rf_clf is not None) # check to ensure the right variable name is used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23a52255f29b24306801a8775d67f19e",
     "grade": false,
     "grade_id": "cell-791d0c1b2ef5f35f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now train each of these classifiers on the training data `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.660917Z",
     "start_time": "2023-09-07T11:10:58.714034Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58dabb662f5c61e387e6b49d0965bf9f",
     "grade": false,
     "grade_id": "cell-3dc58e1c3955fce3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifiers here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.665805Z",
     "start_time": "2023-09-07T11:11:12.662556Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0574412dc171e1fdc6a93ccf6bba4a1",
     "grade": true,
     "grade_id": "cell-17855f34550397f1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(sgd_clf.n_iter_ > 0) # NOTE: n_iter_ will not exist until training\n",
    "assert(dt_clf.n_features_in_ > 0) # NOTE: n_features_in_ will not exist until training\n",
    "assert(rf_clf.n_features_in_ > 0) # NOTE: n_features_in_ will not exist until training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87db8294f9aea8e6cf92da10f11825cd",
     "grade": false,
     "grade_id": "cell-d0de90bcabc83ccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1 Evaluation\n",
    "\n",
    "We start evaluation by just focusing on the Logistic Regression, and see what the accuracy is on the **training data** on which it was optimized on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.680391Z",
     "start_time": "2023-09-07T11:11:12.667951Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af65d5cf47d474a2d240056a4e5b370f",
     "grade": false,
     "grade_id": "cell-555ed48a951e3f77",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# predict the class labels of the linear classifier on the training data\n",
    "\n",
    "y_train_pred = None # store your solution in variable name\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.696240Z",
     "start_time": "2023-09-07T11:11:12.682793Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "920e958316813a9ba50d6338a50b0c20",
     "grade": true,
     "grade_id": "cell-054663ba6b7fac4d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert y_train_pred is not None # check to ensure the right variable name is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9e8e2b987b7f6f99bdb861fc41f78ea",
     "grade": false,
     "grade_id": "cell-ee32c430fe5d85f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To compute the accuracy of the prediction, we can use `accuracy_score` from sklearn. However, you should first show that you know how to implement the accuracy yourself. Therefore implement a function `my_accuracy_score` which behaves like sklearn's `accuracy_score`, but do so without using any functions from sklearn modules. You may use numpy if you want to in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.713102Z",
     "start_time": "2023-09-07T11:11:12.701908Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "825b257e74e5695bb1a668f94b8b3691",
     "grade": false,
     "grade_id": "cell-924779c6846e265c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def my_accuracy_score(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('Sklearns accuracy:', sklearn.metrics.accuracy_score(y_train, y_train_pred))\n",
    "print('    Your accuracy:', my_accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.727919Z",
     "start_time": "2023-09-07T11:11:12.718084Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9e421820966d6f35364d23e42c196e8",
     "grade": true,
     "grade_id": "cell-a8ae49a9a7a2a4de",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "assert(my_accuracy_score(y_train, y_train_pred) == accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# let's also try some dummy values\n",
    "assert math.isclose(my_accuracy_score([1,2,4,2,3,2,2], [4,2,4,4,3,2,1]), 4./7.)\n",
    "# this should also work with numpy arrays\n",
    "assert math.isclose(my_accuracy_score(np.array([1,2,4,2,3,2,2]), np.array([4,2,4,4,3,2,1])), 4./7.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c80f1787e41138e0eaa22c497c012d20",
     "grade": false,
     "grade_id": "cell-8db5b939b2a45d48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ok, let's create a function to put these steps together, so we can easily evaluate any classifier on a given labeled dataset (X,y).\n",
    "As part of the performance statistics, let's report the accuracy and the confusion matrix.\n",
    "You do not need to compute the confusion matrix manually, you can use the builtin function from sklearn for this. You are also free to use sklearn's `accuracy_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.734964Z",
     "start_time": "2023-09-07T11:11:12.729881Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24ba8511e99017721adc3248d63e4dbd",
     "grade": false,
     "grade_id": "cell-9d8e355a50c1a672",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You may want need to import some stuff from sklearn here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def eval_classifier(clf, X, y):\n",
    "    accuracy = None # compute this (you can use sklearn)\n",
    "    confmat = None # compute this (you can use sklearn)\n",
    "\n",
    "    # do something with classifier `clf` here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy, confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily check how all classifiers perform on **the training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.890121Z",
     "start_time": "2023-09-07T11:11:12.738976Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5065abcaedef1d9349643a05c63fa76c",
     "grade": true,
     "grade_id": "cell-160b3b9153373608",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def report_eval(name, accuracy, confmat):\n",
    "    print(f'*** {name} ***')\n",
    "    print(f' confusion matrix:')\n",
    "    print(confmat)\n",
    "    print(f' accuracy: {accuracy:.3f}')\n",
    "    print()\n",
    "\n",
    "print('-- TRAINING data evaluation --')\n",
    "print()\n",
    "\n",
    "# logistic regression\n",
    "sgd_train_accuracy, sgd_train_confmat = eval_classifier(sgd_clf, X_train, y_train)\n",
    "report_eval('Logistic Regression', sgd_train_accuracy, sgd_train_confmat)\n",
    "\n",
    "# decision tree\n",
    "dt_train_accuracy, dt_train_confmat = eval_classifier(dt_clf, X_train, y_train)\n",
    "report_eval('Decision Tree', dt_train_accuracy, dt_train_confmat)\n",
    "\n",
    "# random forest\n",
    "rf_train_accuracy, rf_train_confmat = eval_classifier(rf_clf, X_train, y_train)\n",
    "report_eval('Random Forest', rf_train_accuracy, rf_train_confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82189f41cb714c537aa8ce2c6caf3724",
     "grade": false,
     "grade_id": "cell-693b531bd94807a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the same function to evaluate how each of the three classifiers perform on **the TEST data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.966474Z",
     "start_time": "2023-09-07T11:11:12.891727Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-- TEST data evaluation --')\n",
    "print()\n",
    "\n",
    "# logistic regression\n",
    "sgd_test_accuracy, sgd_test_confmat = eval_classifier(sgd_clf, X_test, y_test)\n",
    "report_eval('Logistic Regression', sgd_test_accuracy, sgd_test_confmat)\n",
    "\n",
    "# decision tree\n",
    "dt_test_accuracy, dt_test_confmat = eval_classifier(dt_clf, X_test, y_test)\n",
    "report_eval('Decision Tree', dt_test_accuracy, dt_test_confmat)\n",
    "\n",
    "# random forest\n",
    "rf_test_accuracy, rf_test_confmat = eval_classifier(rf_clf, X_test, y_test)\n",
    "report_eval('Random Forest', rf_test_accuracy, rf_test_confmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.971339Z",
     "start_time": "2023-09-07T11:11:12.968245Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b67ee4469a30d2dbcaa343d7a2cd2224",
     "grade": true,
     "grade_id": "cell-67278acc92daacc3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please note: The expected outcome for the worst classifier should be better than 54% and the others will perform even better.\n",
    "\n",
    "assert sgd_test_accuracy > 0.54\n",
    "assert dt_test_accuracy > 0.54\n",
    "assert rf_test_accuracy > 0.54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Based on these results, which of these classifiers is overfitting most?\n",
    "\n",
    "Answer by only uncommenting the correct answer in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.975645Z",
     "start_time": "2023-09-07T11:11:12.972661Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67c5243d4dcefd0231f29cd8598d8ee0",
     "grade": false,
     "grade_id": "cell-bae06c57218b6e4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer by uncommenting only the correct option from this block below\n",
    "ANSWER_OVERFITTING_MOST = 'no answer given yet ...'\n",
    "#ANSWER_OVERFITTING_MOST = 'Logistic Regression'\n",
    "#ANSWER_OVERFITTING_MOST = 'Decision Tree'\n",
    "#ANSWER_OVERFITTING_MOST = 'Random Forest'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.979668Z",
     "start_time": "2023-09-07T11:11:12.976898Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56097f1585af03bd18952cd14bfe5dda",
     "grade": true,
     "grade_id": "cell-d0dae5593abdc443",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer: ', ANSWER_OVERFITTING_MOST)\n",
    "\n",
    "# to answer, you should have selected one of the three options ...\n",
    "assert(ANSWER_OVERFITTING_MOST in ('Logistic Regression', 'Decision Tree', 'Random Forest'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db85391a57d14c33f122d84d7e0a1423",
     "grade": false,
     "grade_id": "cell-5fe4f0c3db2d6f6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726347/View\">Lecture 2D - Hyperparameters and Cross-validation</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4a1a97a11527206208c3f671a4f4276",
     "grade": false,
     "grade_id": "cell-ff1255f575b92ea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='fine_tune_your_models'></a>\n",
    "# 6. Fine-tune your models\n",
    "\n",
    "Evaluating on the training data is not giving us a realistic view of the performance of the classifier on the new (test) data. However, we should avoid relying on our test data to perform model selection or hyperparameter optimization.\n",
    "\n",
    "You have learned (from the video and the book) of a better approach to estimate the test performance without using the test data, but by using the training data only (!). Use this strategy to get an expected value and standard deviation for the test accuracy.\n",
    "\n",
    "Note: you can use the default settings that sklearn provides for its implementation of this strategy, only make sure you estimate the `accuracy` and not some other metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:11:12.984589Z",
     "start_time": "2023-09-07T11:11:12.981149Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21824cbc8cb70db4259ccf144843b03d",
     "grade": false,
     "grade_id": "cell-7ea84cd80ed16727",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You may want need to import some stuff from sklearn here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def predict_classifier_test_accuracy(clf, X_train, y_train):\n",
    "    mean_accuracy = None # determine this\n",
    "    stddev_accuracy = None # determine this\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean_accuracy, stddev_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:03.479353Z",
     "start_time": "2023-09-07T11:11:12.986431Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46e772578ad3d9fee0ae9a27248bb7b7",
     "grade": true,
     "grade_id": "cell-8de1c0126364d46f",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see if it works. NOTE: this may take a while ...\n",
    "\n",
    "sgd_mean_accuracy, sgd_stddev_accuracy = predict_classifier_test_accuracy(sgd_clf, X_train, y_train)\n",
    "print('*** Logistic Regression ***')\n",
    "print('    Mean:', sgd_mean_accuracy)\n",
    "print('Std.dev.:', sgd_stddev_accuracy)\n",
    "print()\n",
    "\n",
    "dt_mean_accuracy, dt_stddev_accuracy = predict_classifier_test_accuracy(dt_clf, X_train, y_train)\n",
    "print('*** DT Classifier ***')\n",
    "print('    Mean:', dt_mean_accuracy)\n",
    "print('Std.dev.:', dt_stddev_accuracy)\n",
    "print()\n",
    "\n",
    "rf_mean_accuracy, rf_stddev_accuracy = predict_classifier_test_accuracy(rf_clf, X_train, y_train)\n",
    "print('*** RF Classifier ***')\n",
    "print('    Mean:', rf_mean_accuracy)\n",
    "print('Std.dev.:', rf_stddev_accuracy)\n",
    "print()\n",
    "\n",
    "# Compare expected accuracy to true test accuracy. Difference should be small!\n",
    "print('Comparing to test accuracy ...')\n",
    "assert(np.abs(sgd_mean_accuracy - sgd_test_accuracy) < 0.1)\n",
    "assert(np.abs(dt_mean_accuracy - dt_test_accuracy) < 0.1)\n",
    "assert(np.abs(rf_mean_accuracy - rf_test_accuracy) < 0.1)\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ef18628d6531756e7edfc80d7d9e13d",
     "grade": false,
     "grade_id": "cell-6f115cce9cbb8e77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='present_your_solution'></a>\n",
    "# 7. Present your solution\n",
    "\n",
    "It is important to not only look at statistics, but also to confirm yourself that the statistics make sense.\n",
    "Let's visualize the classification results by drawing the predicted labels back on the respective images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:03.497498Z",
     "start_time": "2023-09-07T11:12:03.483369Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "def plot_image_classification_results(clf, Is, img_idx, Ps_test, labels_test, points_test, imgids_test):\n",
    "    mask = imgids_test == img_idx\n",
    "\n",
    "    y_test_pred = clf.predict(Ps_test[mask])\n",
    "    y_test_pred_prob = clf.predict_proba(Ps_test[mask])\n",
    "    points = points_test[mask,:]\n",
    "\n",
    "    confmat = confusion_matrix(labels_test[mask], y_test_pred)\n",
    "    accuracy = accuracy_score(labels_test[mask], y_test_pred)\n",
    "    #jaccard = sklearn.metrics.jaccard_score(labels_test[mask], y_test_pred, average='macro')\n",
    "\n",
    "    print(f' confusion matrix:')\n",
    "    print(confmat)\n",
    "    print(f' accuracy: {accuracy:.3f}')\n",
    "    #print(f'  jaccard: {jaccard:.3f}')\n",
    "\n",
    "    # Post-processing of result:\n",
    "    #  select the patches for which the classifier is most confident that it belongs to the tip or end class\n",
    "    best_idx1 = y_test_pred_prob[:,1].argmax() # id of most confident 'tip' patch\n",
    "    best_idx2 = y_test_pred_prob[:,2].argmax() # id of most confident 'end' patch\n",
    "    \n",
    "    # load image\n",
    "    I = Is[img_idx]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(I)\n",
    "    plt.plot(points[y_test_pred==0, 0], points[y_test_pred==0, 1], '.r')\n",
    "    plt.plot(points[y_test_pred==3, 0], points[y_test_pred==3, 1], '.y')\n",
    "    plt.plot(points[y_test_pred==1, 0], points[y_test_pred==1, 1], '.g')\n",
    "    plt.plot(points[y_test_pred==2, 0], points[y_test_pred==2, 1], '.b')\n",
    "    plt.plot(points[(best_idx1, best_idx2), 0], points[(best_idx1, best_idx2), 1], 'c-', linewidth=3)\n",
    "    plt.plot(points[best_idx1, 0], points[best_idx1, 1], 'cs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:03.782389Z",
     "start_time": "2023-09-07T11:12:03.500232Z"
    }
   },
   "outputs": [],
   "source": [
    "# On TRAIN data\n",
    "train_img_idxs = np.unique(imgids_train)\n",
    "classifiers = {'Logistic Regression': sgd_clf, 'Random Forest': rf_clf, 'Decision-Tree': dt_clf}\n",
    "\n",
    "def plot_nth_train_result(clf, n, Is, train_img_idxs, X_train, y_train, points_train, imgids_train):\n",
    "    plot_image_classification_results(clf, Is, train_img_idxs[n], X_train, y_train, points_train, imgids_train)\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_nth_train_result, \n",
    "    clf=classifiers, \n",
    "    n=(0, len(train_img_idxs) - 1), \n",
    "    Is=ipywidgets.fixed(Is), \n",
    "    train_img_idxs=ipywidgets.fixed(train_img_idxs), \n",
    "    X_train=ipywidgets.fixed(X_train), \n",
    "    y_train=ipywidgets.fixed(y_train), \n",
    "    points_train=ipywidgets.fixed(points_train), \n",
    "    imgids_train=ipywidgets.fixed(imgids_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:04.066179Z",
     "start_time": "2023-09-07T11:12:03.784614Z"
    }
   },
   "outputs": [],
   "source": [
    "# On TEST data\n",
    "test_img_idxs = np.unique(imgids_test)\n",
    "classifiers = {'Logistic Regression': sgd_clf, 'Random Forest': rf_clf, 'Decision-Tree': dt_clf}\n",
    "\n",
    "def plot_nth_test_result(clf, n, Is, test_img_idxs, X_test, y_test, points_test, imgids_test):\n",
    "    plot_image_classification_results(clf, Is, test_img_idxs[n], X_test, y_test, points_test, imgids_test)\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_nth_test_result, \n",
    "    clf=classifiers, \n",
    "    n=(0, len(test_img_idxs) - 1), \n",
    "    Is=ipywidgets.fixed(Is), \n",
    "    test_img_idxs=ipywidgets.fixed(test_img_idxs), \n",
    "    X_test=ipywidgets.fixed(X_test), \n",
    "    y_test=ipywidgets.fixed(y_test), \n",
    "    points_test=ipywidgets.fixed(points_test), \n",
    "    imgids_test=ipywidgets.fixed(imgids_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea8f2ae6ec08fc2003d5a63f8dceaa3b",
     "grade": false,
     "grade_id": "cell-a617af23967d8d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Confirm with visual inspection if the evaluation statistics make sense.\n",
    "For instance, try to see if you find any patterns in the errors that the classifiers make,\n",
    "and under what conditions most errors occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc7134db96ec63944e4e08f4e32ccbfc",
     "grade": false,
     "grade_id": "cell-e4a6cd674165639f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <a id='maintain_your_system'></a>\n",
    "# 8. Launch, monitor, and maintain your system\n",
    "\n",
    "Finally, you would apply your system on the real robot, and study if your solution properly addresses the task you set out to solve over a longer period. As a result, you might revisit some of the previous steps if you encounter new issues, e.g. low detection scores, high latency due to too large computational requirements, or changes in the environmental conditions (a new pen, or background).\n",
    "\n",
    "For this assignment, we will use this step to perform some follow-up experiments with the system, and also ask you to collect your own dataset with your lab partner.\n",
    "\n",
    "\n",
    "\n",
    "### 8.1 Test on uniformly sampled grid (strategy 1)\n",
    "\n",
    "It may look like the classification task has been solved, however our procedure to extract the patches from the training and testing images oversamples non-background locations. While this results in more balanced classes, this is not realistic for how the detector could be used in practice. In a true test case, we don't already know where the pen is located. We should therefore re-investigate our test images, and check what the test performance is on uniformly distributed patches.\n",
    "\n",
    "Let's use the classifiers that were already trained on strategy 2, and see what their test performance is on strategy 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:04.587561Z",
     "start_time": "2023-09-07T11:12:04.067999Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test2, y_test2, points_test2, imgids_test2 = extract_multiple_images(Is, test_imgs, annots, strategy=1)\n",
    "\n",
    "print('Overal class count:')\n",
    "print(count_classes(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:04.858124Z",
     "start_time": "2023-09-07T11:12:04.589801Z"
    }
   },
   "outputs": [],
   "source": [
    "# On TEST data\n",
    "test_img_idxs2 = np.unique(imgids_test2)\n",
    "classifiers = {'Logistic Regression': sgd_clf, 'Random Forest': rf_clf, 'Decision-Tree': dt_clf}\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_nth_test_result, \n",
    "    clf=classifiers, \n",
    "    n=(0, len(test_img_idxs2) - 1), \n",
    "    Is=ipywidgets.fixed(Is), \n",
    "    test_img_idxs=ipywidgets.fixed(test_img_idxs2), \n",
    "    X_test=ipywidgets.fixed(X_test2), \n",
    "    y_test=ipywidgets.fixed(y_test2), \n",
    "    points_test=ipywidgets.fixed(points_test2), \n",
    "    imgids_test=ipywidgets.fixed(imgids_test2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, if you compare the accuracy between the sampling strategies for the same test image and same classifier, you may notice that often the accuracy on the uniform grid (strategy 1) is higher. The test classes are so unbalanced, and as long as a classifier can get most of the \"easy\" background patches correct, it will obtain a high accuracy.\n",
    "\n",
    "On the other hand, visually the classifiers seem to make many more mistakes, and we see that the classifiers often misclassify large regions in the image. Maybe the classifiers are too 'eager' to classify patches as pen-parts as in the training data these classes were more likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ca9c26085d0350fec027634f2252c77",
     "grade": false,
     "grade_id": "cell-cd4c4c61971ca855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 8.2 Timing evaluation\n",
    "\n",
    "Let's also try to see how we can use the jupyter notebook to get some quick and dirty timing results for when you are running some experiments. Keeping an eye on how long a particular computation steps takes can be important when planning your experiments, and to figure out how your problems scale with larger and more complex datasets.\n",
    "\n",
    "We'll focus on timing the training and testing phases of the classifiers. Note that a proper timing evaluation requires running each procedure multiple times, but here we will skip this step and only fit each model once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: you can use the special command `%%time` at the start of a notebook cell to report the amount of time in seconds in took to execute a cell.\n",
    "To illustrate this, we have some simple examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:04.861835Z",
     "start_time": "2023-09-07T11:12:04.859407Z"
    }
   },
   "outputs": [],
   "source": [
    "def slow_function():\n",
    "    # this is a dummy function which does nothing, but just waits for 0.5 seconds to illustrate time profiling\n",
    "    import time\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:05.271777Z",
     "start_time": "2023-09-07T11:12:04.864182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of using %time\n",
    "#   Note that %time is a \"magic\" jupyter notebook command, and not actual python code\n",
    "#   the %time command automatically times the execution speed of the command following it\n",
    "\n",
    "%time slow_function() # timed\n",
    "slow_function() # not timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:05.684331Z",
     "start_time": "2023-09-07T11:12:05.276529Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Example of using %%time\n",
    "#   Note that %%time is a \"magic\" cell command, which applies to the whole cell rather than a single line.\n",
    "#   Note how the cell STARTS with the %%time command\n",
    "\n",
    "# whole cell is timed\n",
    "slow_function()\n",
    "slow_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to train each classifier on your computer?\n",
    "Report your results here (it is sufficient to print the timing results in the output, you don't need to store these numbers in variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.436399Z",
     "start_time": "2023-09-07T11:12:05.688039Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d43e45abdf06a6a4aa11d6cf293fb10",
     "grade": false,
     "grade_id": "cell-4f1431cc3b4e5425",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb87309c5016f8a4a1f87df06fb5892a",
     "grade": false,
     "grade_id": "cell-44961510919d57b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now test the how long it takes each classifier to make a prediction on all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.613194Z",
     "start_time": "2023-09-07T11:12:19.439723Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c3d886a73356979ad11cee23b44d18",
     "grade": false,
     "grade_id": "cell-77df663ca235e616",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Based on these time results, which method will have the least latency when processing images sequentially on a live video stream from the robot?\n",
    "\n",
    "Answer by uncommenting only the correct option from the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.618229Z",
     "start_time": "2023-09-07T11:12:19.614941Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e15a18f8d00e9b4d627b249d3e62d9",
     "grade": false,
     "grade_id": "cell-4cd7eb511974fb9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer by uncommenting only the correct option from this block below\n",
    "ANSWER_LOWEST_LATENCY = 'no answer given yet ...'\n",
    "#ANSWER_LOWEST_LATENCY = 'Logistic Regression'\n",
    "#ANSWER_LOWEST_LATENCY = 'Decision Tree'\n",
    "#ANSWER_LOWEST_LATENCY = 'Random Forest'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.623308Z",
     "start_time": "2023-09-07T11:12:19.620419Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b90882851cdf16cd17bb0280987c6a94",
     "grade": true,
     "grade_id": "cell-2b06f9e66f11bba2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer: ', ANSWER_LOWEST_LATENCY)\n",
    "\n",
    "# to answer, you should have selected one of the three options ...\n",
    "assert(ANSWER_LOWEST_LATENCY in ('Logistic Regression', 'Decision Tree', 'Random Forest'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Train and test models with your own data\n",
    "\n",
    "The final task of this practicum is that you and your lab partner do your *own* data collection and annotation.\n",
    "\n",
    "The following instructions should be done *independently* by both lab partners:\n",
    "* Collect several images of your pen with your phone, the resoution should be at least 1024 pixels in width and height.\n",
    "* Take the pictures from above your desk from a more or less fixed distance to the pen, similar to the images used in the previous experiments.\n",
    "* Make sure you rotate the pen everytime such that it always appears at slightly different angles and locations in the images.\n",
    "* Copy the images from your phone to a new subfolder in the `images/` directory. This could be done via a usb cable, wireless transfer, or some cloud service that you can access or your phone or computer. I'm sure you'll be able to figure something out ;) More specifically:\n",
    "    * place the images from the first lab partner in to the `images/lab_partner1/` directory\n",
    "    * place the images from the second lab partner in to the `images/lab_partner2/` directory\n",
    "    * (see how is lab partner 1 and how is lab partner 2 in the first code cell at the top of this notebook)\n",
    "* Annotate the start and end points of the pen in all iamges using the provided `Data annotator.ipynb`, which will also resize the images such that the max height or width of 1024 pixels. This way, all extract image $100 \\times 100$ image patches should show more or less a similar size.\n",
    "* Ensure you have at least about 20 images for training, and 10 images for testing.\n",
    "* Exchange your annotated dataset with your lab partner, such that you both have the same filled image directories on your computers.\n",
    "\n",
    "**NOTE:** We will ask you to upload your own annotated image directories to get with this worked out notebook. We may share your datasets with other students in this course for a follow-up experiment. Make sure you have **no sensitive or unappropriate content** in the images that you use, and that you are okay with others seeing them too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93689624cc06fc30207fc4e25b74c0a0",
     "grade": false,
     "grade_id": "cell-a335c7723b778000",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### 8.3.1 Train a classifier on the images of lab partner 1\n",
    "\n",
    "Using the data of the first lab partner,\n",
    "* Prepare a training and testing dataset, similar to what we did before (using the 2nd patch sampling strategy). You can use all the functions that have been defined above, you don't need to redefine those.\n",
    "* On this dataset select a classifier, and compute test performance.\n",
    "* Report the accuracy and confusion matrix of the selected classifier on the test dataset.\n",
    "\n",
    "You are free to add cells below to complete this task, we'll only add a check that the images are found in the correct directory.\n",
    "\n",
    "**Note**: Please don't directly copy the previous cells! This will lead to some malfunctions in the autograder. You should add cells and copy the code inside the previous cells into the new cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.627579Z",
     "start_time": "2023-09-07T11:12:19.624760Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c2ad3ae15851bd8c0be45d10fc3c3e",
     "grade": false,
     "grade_id": "cell-fc0cb07f2676b8da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "partner1_filenames = list_images('images/lab_partner1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.878331Z",
     "start_time": "2023-09-07T11:12:19.629582Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e61b0c7c32d483930636d78267a4b29b",
     "grade": true,
     "grade_id": "cell-af01ca7f2386333a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(partner1_filenames) >= 30) # we need at least 30 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a906e7456ffd9f893c0f70482ab89872",
     "grade": false,
     "grade_id": "cell-e8d1516a4f8697ee",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### 8.3.2 Train a classifier on the images of lab partner 2\n",
    "\n",
    "Do the same for the images the second lab partner here below. Same rules apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.884784Z",
     "start_time": "2023-09-07T11:12:19.884757Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ad7d2cc17da91f4917b67c5c9cf9880",
     "grade": false,
     "grade_id": "cell-f0780531101dc1a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "partner2_filenames = list_images('images/lab_partner2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:12:19.886534Z",
     "start_time": "2023-09-07T11:12:19.886512Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(partner2_filenames) >= 30) # we need at least 30 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8023e61194873be835da69de684bdf00",
     "grade": false,
     "grade_id": "cell-7688bd5dd9be36ce",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "As the final step, let's see how well your classifier generalizes to new data that has been collected independently of the training data. This is what often happens when we use pre-trained classifiers from another lab, or when a robot is placed in a different environment than where it was developed.\n",
    "\n",
    "Add code below to take the trained classifier on the **training data** of **lab partner 2**, and evaluate how well it performs on the **test data** of **lab partner 1**.\n",
    "Apart from some statistics, also visually inspect the classification result.\n",
    "Finish with a Markdown cell where you explain your observations: how well did the classifier perform on the other dataset? Is that better or worse than you both expected? Can you see what kind of patches it is misclassifying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finishes this week's practicum! \n",
    "Please zip your saved notebook with your solutions, and don't forget to include this zip also your and your labpartner's annotated datasets.\n",
    "Each dataset directory should contain the annotations you created, and the images you collected (but only the ones resized to max dimensions of 1024 pixels by the \"Data annotator\" tool, not your huge original high-resolution pictures!).\n",
    "Hopefully if we collect a large and diverse dataset from all students, we can make our robot more robustly pickup pens or its collections :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
