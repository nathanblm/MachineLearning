{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656cf86f",
   "metadata": {},
   "source": [
    "# RO47002 Machine Learning for Robotics\n",
    "* (c) TU Delft, 2024\n",
    "* Period: 2024-2025, Q1\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/682421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b4d41",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"\"\n",
    "STUDENT_NAME1 = \"\"\n",
    "STUDENT_NUMBER1 = \"\"\n",
    "STUDENT_NAME2 = \"\"\n",
    "STUDENT_NUMBER2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725d6b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3f76d6a626db81c484191482b101edb",
     "grade": true,
     "grade_id": "cell-c35e4c8223095209",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e31aee",
   "metadata": {},
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you and your lab partner alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled practicum hours to ask a TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c8fc9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a4e57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ff4cf1d8acbba5e88cc9ae9f5de788f",
     "grade": false,
     "grade_id": "cell-33ce90de5ea4a8c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practicum 5\n",
    "\n",
    "* Topics: Dimensionality reduction, clustering\n",
    "* Before performing this practicum, work through **Book chapter(s): 8, 9**\n",
    "* **Deadline**: Monday, October 7, 2024, 23:59\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* This lab assignment consists of two parts:\n",
    "\n",
    "### Part 1 - Dimensionality Reduction\n",
    "* Understanding Principal Component Analysis (PCA) transformations using on a 3D toy dataset\n",
    "* Implementing PCA project and reconstruction yourself\n",
    "* Using PCA for data compression\n",
    "* Using PCA on a real Pedestrian image dataset, and computing the \"Eigen-Pedestrians\"\n",
    "\n",
    "### Part 2 - Clustering\n",
    "* Using K-Means on a toy dataset\n",
    "* Implementing the basic K-Means algorithm yourself\n",
    "* Using a Gaussian Mixture Model (GMM) for clustering\n",
    "* Comparing K-Means and GMM for outlier detection\n",
    "* Application: *Meeting and recognizing human faces*\n",
    "* A simple semi-supervised approach to improve clustering\n",
    "\n",
    "# Part 1\n",
    "\n",
    "### Setup common python stuff\n",
    "We will start by loading a few common python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640856d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d740ce3",
   "metadata": {},
   "source": [
    "## 1.1 PCA on 3D Toy Data\n",
    "\n",
    "We will first create a small 3D \"toy\" dataset to investigate PCA as a dimensionality reduction technique.\n",
    "We'll  treat this data just as samples from some distribution over the 3D feaure space.\n",
    "\n",
    "For now, we will not consider that different samples have different class labels.\n",
    "There could be various reasons why we would want to consider PCA to consider such a data distribution. For instance:\n",
    "- These could all be samples from the same class and we want to understand how to model the class-conditional distribution in a Bayesian classifier.\n",
    "- We might know that these will be samples from mulitple classes, but we just do not know the class labels yet (maybe these still need to be annotated) but sill wish to compress the dataset size.\n",
    "- We might want to create a 2D plot of the samples, so we can get an intuition of the data in the higher dimensional space. E.g. maybe we can already determine if the classes are easily separable or not, and how their data is distributed (e.g. is the distribution skewed, symmetric, uncorrelated, etc.). A word of caution though: if the classes are easily separable in the 2D plot after a linear projection, they'll also be seperable by a linear classifier in the original higher dimensional space; However if they are not easily separable in 2D space, they could be seperable in a 2+ dimensional space, so this doesn't give a definitive answer.\n",
    "\n",
    "Our goal here is to use understand how PCA preserves the variance of the data by projecting to a lower dimensional space, and how we can perform the inverse projection from this projection back to the original 3D space to reconstruct the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad002076",
   "metadata": {},
   "source": [
    "### 1.1.1 Create and explore the 3D toy dataset\n",
    "\n",
    "The block below creates the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "mask = (y != 0)\n",
    "X = X[mask,:]\n",
    "y = y[mask]\n",
    "\n",
    "# make data a bit more interesting\n",
    "X = X[:,:3] # only use 3 dimensions\n",
    "X[:,1] *= 0.3 # scale down 2nd dimension\n",
    "X[:,0] += X[:,1] * 0.8\n",
    "X[:,2] *= 1.5\n",
    "\n",
    "y = (y == 2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba41475",
   "metadata": {},
   "source": [
    "How could you determine which features are correlated, and what features contain most of the variance in the data? One way is to compute and report some statistics on the features.\n",
    "\n",
    "Use the code block below to compute the variance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443e044",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ca2b8857aba5484f6c244c64fa02b14",
     "grade": false,
     "grade_id": "cell-9ed545710c9ad8e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the variance of each feature, and print them to the output for instance.\n",
    "# You'll use the result to answer the question in the next code block.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c6fcd",
   "metadata": {},
   "source": [
    "Based on your calculations, order the 3 features from most to least variance.\n",
    "\n",
    "Use the variable `FEATURE_ORDER_MOST_TO_LEAST_VARIANCE` to give your answer by listing the feature dimensions (0, 1, 2) in order of decreasing variance.\n",
    "For instance, if you answer that feature 0 has most variance, feature 1 the second most, and feature 2 the least variance, answer: `FEATURE_ORDER_MOST_TO_LEAST_VARIANCE = [0, 1, 2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d5d90",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6a3ff6cbc6bf20d15d67f7bdfe032e5",
     "grade": false,
     "grade_id": "cell-8e4433e32e4b2fd4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here and put the numbers 0, 1, 2 in the right order\n",
    "FEATURE_ORDER_MOST_TO_LEAST_VARIANCE = [-1, -1, -1]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62968952",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9207992a33142222534f724985823aac",
     "grade": true,
     "grade_id": "cell-699de6a1d97725b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all numbers 0, 1, 2 occur once in your answer\n",
    "assert(len(FEATURE_ORDER_MOST_TO_LEAST_VARIANCE) == 3)\n",
    "assert(np.all(np.bincount(FEATURE_ORDER_MOST_TO_LEAST_VARIANCE) == [1,1,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a720f",
   "metadata": {},
   "source": [
    "The statistics are useful, but don't show the structure of the samples directly.\n",
    "\n",
    "Since this is a 3D dataset, we can also try to visualize all 3 features at once in a 3D plot\n",
    "which can be rotated around to better understandin the underlying structure.\n",
    "The provided code below sets up the [3D plotting interface of matplotlib](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html), and uses the ipython widgets to allow you to adapt the viewing angle and see the data from different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def make_3d_plot_axes_equal(ax):\n",
    "    \"\"\" Utility function to make axes equally scaled for 3D plots in matplotlib.\n",
    "        Note that for 2D plots we can simply use ax.axes('equal'),\n",
    "        but unfortunately this doesn't work for 3D plots, so we use this utility function.\n",
    "        \n",
    "        Inspired by: https://stackoverflow.com/a/31364297\n",
    "    \"\"\" \n",
    "    \n",
    "    ax_limits = np.array([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()]).T\n",
    "    \n",
    "    m = ax_limits.mean(axis=0)\n",
    "    max_range = (ax_limits - m).max();\n",
    "    \n",
    "    ax.set_xlim(m[0] - max_range, m[0] + max_range)\n",
    "    ax.set_ylim(m[1] - max_range, m[1] + max_range)\n",
    "    ax.set_zlim(m[2] - max_range, m[2] + max_range)\n",
    "    \n",
    "\n",
    "def plot_3d_data(X, view_angle1, view_angle2, label_name='dim'):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.view_init(view_angle1, view_angle2)\n",
    "    \n",
    "    ax.scatter(X[:,0], X[:,1], X[:,2], s=5., alpha=0.7)\n",
    "\n",
    "    plt.xlabel(label_name+' 0')\n",
    "    plt.ylabel(label_name+' 1')\n",
    "    ax.zaxis.set_label_text(label_name+' 2') # no plt.zlabel() :-/\n",
    "\n",
    "    # ensure 3D plot has equally scaled axes\n",
    "    make_3d_plot_axes_equal(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72119512",
   "metadata": {},
   "source": [
    "The 3D plot allows us to get a good feeling of how the data is distributed in the space, but it can be hard to really read off particular feature values for any sample.\n",
    "Alternatively, we could have projected the 3D to a 2D plane by only plotting 2 feature dimensions at once, ignoring the third. With 3 features, there are 3 possible feature combinations to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_on_axes(X, feat_hor, feat_ver):\n",
    "    plt.plot(X[:, feat_hor], X[:, feat_ver], '.')\n",
    "    plt.xlabel(f'feature {feat_hor}')\n",
    "    plt.ylabel(f'feature {feat_ver}')\n",
    "    plt.axis('equal')\n",
    "    plt.grid('on')\n",
    "\n",
    "def plot_axis_combinations(X):\n",
    "    plt.subplot(1,3,1)\n",
    "    plot_data_on_axes(X, feat_hor=0, feat_ver=1)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plot_data_on_axes(X, feat_hor=0, feat_ver=2)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plot_data_on_axes(X, feat_hor=1, feat_ver=2)\n",
    "    \n",
    "plt.figure(figsize=(14,4))\n",
    "plot_axis_combinations(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803847a3",
   "metadata": {},
   "source": [
    "**Q**: Based on these plots, which of these feature pairs are most strongly correlated?\n",
    "\n",
    "Answer in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2f434",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a0d9f9465e74938aa72086f371d3d2f",
     "grade": false,
     "grade_id": "cell-19f02c41cbbaa8fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here by the indices (0, 1 or 2) of the two strongest correlated features\n",
    "#   NOTE: order doesn't matter for this answer\n",
    "MOST_STRONLGY_CORRELATED_FEATURE_PAIR = (-1, -1)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2edbc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8477ffa73a1f9a26733d7b58199f16c",
     "grade": true,
     "grade_id": "cell-4ed52fb4030b6979",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(MOST_STRONLGY_CORRELATED_FEATURE_PAIR) == 2)\n",
    "assert(MOST_STRONLGY_CORRELATED_FEATURE_PAIR[0] in (0,1,2))\n",
    "assert(MOST_STRONLGY_CORRELATED_FEATURE_PAIR[1] in (0,1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a1c3e",
   "metadata": {},
   "source": [
    "### 1.1.2 Transforming the data with Principal Component Analysis\n",
    "\n",
    "The 2D plots give a better picture of the statstical relation between feature pairs, but all of them give you an incomplete picture because they ignore some aspects of the data. With higher dimensional data, there are many more possible feature combinations to explore. And, we can't plot all features in an interactive plot for more the 3 dimensions either.\n",
    "\n",
    "PCA allows us to find a linear transformation of the data, i.e. by performing a PCA projection we create a  transformed dataset where each feature is a linear combination of the original features.\n",
    "In the projected representation, the first dimension will capture most of the data variance, and the last dimension the least amount of variance.\n",
    "\n",
    "By only keeping the first few PCA dimensions, we can thus reduce high dimensional data to a smaller number of dimensions which still contain as much variations in the data as possibly could be kept with a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbbabd5",
   "metadata": {},
   "source": [
    "Use sklearn's PCA implementation to fit and transform the data $X$.\n",
    "For now, just use the default PCA options, don't set any keywords in the constructor.\n",
    "\n",
    "Remember that most of the intuition and basic usage of sklearn's PCA have been explained in Chapter 8 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7de7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "148942e624f05acdcca5b6402c96c5a5",
     "grade": false,
     "grade_id": "cell-dac37007b8d673f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca = None # store the PCA estimator, we need it later on.\n",
    "X_pca = None # store the result of the PCA projection of X in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cea4fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f7dccf93b6ee5a97e6b6bf5f11c48e2",
     "grade": true,
     "grade_id": "cell-84950c6c8a45795e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca.shape == (100, 3)) # 100 samples, 3 dimensions after projection\n",
    "assert(not pca is None)\n",
    "assert(pca.__class__.__name__ == 'PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba763bc",
   "metadata": {},
   "source": [
    "As we did with the original data features, determine again how much variance there is in each of the 3 dimensions after PCA transformation, and order these dimensions from most to least variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b0c13",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6589d5a9f6b763dfdf88c8abef0bc4a1",
     "grade": false,
     "grade_id": "cell-43e4f68273047489",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here and put the numbers 0, 1, 2 in the right order\n",
    "PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE = [-1, -1, -1]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03a8a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d46ec60ff94b0df1782befc238ead896",
     "grade": true,
     "grade_id": "cell-252d2593b60dc1e5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all numbers 0, 1, 2 occur once in your answer\n",
    "assert(len(PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE) == 3)\n",
    "assert(np.all(np.bincount(PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE) == [1,1,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c194029",
   "metadata": {},
   "source": [
    "The data after the PCA project still has 3 dimensions. Therefore, we can again visualize the data using 3D or 2D plots, like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ed401",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plot_axis_combinations(X_pca)\n",
    "\n",
    "# Note: using X_pca here\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X_pca, view_angle1, view_angle2, label_name='PCA proj.'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba33cd7",
   "metadata": {},
   "source": [
    "\n",
    "Notice how the order and variance of each dimension after PCA projection is different from those of the original 3 feature dimensions (compare the data statistics here to those of the original data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9cbf9",
   "metadata": {},
   "source": [
    "### 1.1.3 Visualizing the PCA components\n",
    "\n",
    "The PCA object has computed two important statistics,\n",
    "the `mean` of the data in the feature space,\n",
    "and the `principal components`.\n",
    "The principal components define a new *orthonormal basis* in the original feature space,\n",
    "located around the mean of the data.\n",
    "An orthonormal basis means that the basis vectors are\n",
    "1. all perpendicular to eachother\n",
    "2. all have unit length\n",
    "\n",
    "We can therefore regard it as a translation of the original feature, to subtract the mean, plus a rotation (and possibly with mirroring of some axes, which would just swap the sign in one of the dimensions).\n",
    "\n",
    "Let's visualize the mean and principal components in the original feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_pca_components(pca, X, view_angle1, view_angle2, label_name='dim'):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # set view angle\n",
    "    ax.view_init(view_angle1, view_angle2)\n",
    "\n",
    "    # plot data\n",
    "    ax.scatter(X[:,0], X[:,1], X[:,2], s=5., alpha=0.3)\n",
    "\n",
    "    # get the data mean (a 3D vector) from the pca object, and plot it as star\n",
    "    m = pca.mean_\n",
    "    ax.plot((m[0],), (m[1],), (m[2],), 'k*', label='mean')\n",
    "    \n",
    "    # get each of the 3 pca components from the pca object, and plot it as a vector from the mean\n",
    "    for c in range(pca.components_.shape[0]):\n",
    "        comp = pca.components_[c]\n",
    "        color = 'rgb'[c]\n",
    "        p1 = m + comp\n",
    "        ax.plot((m[0], p1[0]), (m[1], p1[1]), (m[2], p1[2]), color+'-', label=f'PCA comp. {c}')\n",
    "    \n",
    "    # give all the axes a nice name\n",
    "    plt.xlabel(label_name+' 0')\n",
    "    plt.ylabel(label_name+' 1')\n",
    "    ax.zaxis.set_label_text(label_name+' 2') # no plt.zlabel() :-/\n",
    "    \n",
    "    # ensure 3D plot has equally scaled axes\n",
    "    make_3d_plot_axes_equal(ax)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca, X, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e224854",
   "metadata": {},
   "source": [
    "Let's implement some tests to show that the computed PCA components (`pca.components_`) indeed form an orthonormal basis. That is\n",
    "\n",
    "1. complete the function `component_is_unit_length(component)` which only returns True if the given vector `component` has unit length, and returns False otherwise.\n",
    "\n",
    "2. complete the function `components_are_orthogonal(comp1, comp2)` which only returns True if the two given vectors `comp1` and `comp2` are perpendicular to each other (*Hint*: what should the dot product of two perpendicular vectors be?)\n",
    "\n",
    "We'll then apply your functions to all PCA components and pairs of components.\n",
    "Note that for these tests you always need to allow for some small tolerance for small deviations of the desired outout, e.g. in the order of 10^-3 . The calculations of the computer have finite precision, so computed the length vector of a unit vector might actually be 1.000001, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65982ae1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26e4f705792615f4d8d17ac44d7c4772",
     "grade": false,
     "grade_id": "cell-b509cb77060d5487",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def component_is_unit_length(component):\n",
    "    \"\"\" Test if the length of a given component is 1.0\n",
    "        NOTE: you can allow some tolerance to numeric imprecision,\n",
    "        e.g. you can return True if the length l is between (0.999) < l < (1.001)\n",
    "        \n",
    "        Input: component - a vector\n",
    "        Output: is_unit_length - a boolean (True or False)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return is_unit_length\n",
    "\n",
    "def components_are_orthogonal(comp1, comp2):\n",
    "    \"\"\" Test two components are orthogonal.\n",
    "        NOTE: As in component_is_unit_length(), some tolerance to numeric imprecision is allowed\n",
    "        \n",
    "        Input: comp1 - a vector\n",
    "        Input: comp2 - a vector\n",
    "        Output: are_orthogonal - a boolean (True or False)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return are_orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4eba2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "523392313e11fc29496da098cf4a2678",
     "grade": true,
     "grade_id": "cell-a9a9d36be1ac3d3e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if each of the 3 PCA components have unit length\n",
    "for j in range(3):\n",
    "    comp = pca.components_[j]\n",
    "    \n",
    "    # your function is called here\n",
    "    is_unit_length = component_is_unit_length(comp)\n",
    "    \n",
    "    assert(is_unit_length in (False, True))\n",
    "    print(f'PCA component {j} has unit length:', is_unit_length)\n",
    "\n",
    "# Test if each pair of PCA components are orthogonal\n",
    "for j in range(3):\n",
    "    for k in range(j+1, 3):\n",
    "        comp1 = pca.components_[j]\n",
    "        comp2 = pca.components_[k]\n",
    "        \n",
    "        # your function is called here\n",
    "        are_orthogonal = components_are_orthogonal(comp1, comp2)\n",
    "        \n",
    "        assert(are_orthogonal in (False, True))\n",
    "        print(f'PCA components {j} and {k} are orthogonal:', are_orthogonal)\n",
    "\n",
    "# Some additional checks to make sure these functions perform correctly\n",
    "\n",
    "# test unit-length and non-unit-length components\n",
    "comp_x = np.array([1.0, 0.0, 0.0])\n",
    "comp_y = np.array([0.0, 1.0, 0.0])\n",
    "assert(components_are_orthogonal(comp_x, comp_y) == True)\n",
    "assert(components_are_orthogonal(comp_y*3, comp_x*-2) == True)\n",
    "# no non-zero vector is orthogonal to itself\n",
    "assert(components_are_orthogonal(comp_x, comp_x) == False)\n",
    "assert(components_are_orthogonal(comp_y, comp_y) == False)\n",
    "\n",
    "assert(component_is_unit_length(comp_x) == True)\n",
    "assert(component_is_unit_length(-comp_y) == True)\n",
    "assert(component_is_unit_length(comp_x*0.9999) == True) # within tolerance\n",
    "assert(component_is_unit_length(-comp_y*1.0001) == True) # within tolerance\n",
    "assert(component_is_unit_length(comp_x*0.99) == False) # outside tolerance\n",
    "assert(component_is_unit_length(comp_y*1.01) == False) # outside tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73699af9",
   "metadata": {},
   "source": [
    "### 1.1.4 Projecting to lower dimensional spaces\n",
    "\n",
    "To linearly project the $M$-dimensional data to only $D$-dimensions, i.e. transform $N \\times M$ matrix $X$ to a $N \\times D$ matrix $X_{pca}$, while maintaining as much variance in the data as possible.\n",
    "Construct below the following projections:\n",
    "\n",
    "* `X_pca2` the 2-dimensional PCA embedding of X\n",
    "* `X_pca1` the 1-dimensional PCA embedding of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb4afc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65c7f6cb3f998fe8fc417bc063d163fb",
     "grade": false,
     "grade_id": "cell-f0136c384c01c6a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca2 = None # estimator for 2-dimensional PCA\n",
    "X_pca2 = None # projection to 2-dimensional space\n",
    "\n",
    "pca1 = None # estimator for 1-dimensional PCA\n",
    "X_pca1 = None # projection to 1-dimensional space\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(X_pca2.shape, X_pca1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ae675",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "272ea76b8a5ccb41cf7794ec8cd372b4",
     "grade": true,
     "grade_id": "cell-ae2b961666c42f92",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca2.shape == (100,2))\n",
    "assert(X_pca1.shape == (100,1))\n",
    "assert(not pca2 is None)\n",
    "assert(not pca1 is None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ca03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "#plot_axis_combinations(X_pca2_)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(X_pca2[:,0], X_pca2[:,1], '.')\n",
    "plt.title('2D projection')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(X_pca1, np.zeros(X_pca1.shape), '.')\n",
    "plt.title('1D projection')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96825aad",
   "metadata": {},
   "source": [
    "### 1.1.5 Back-projecting to the original space\n",
    "\n",
    "Finally, let's try to reconstruct the data from the PCA projects back in the original $M$-dimensional feature space:\n",
    "\n",
    "* `X_recon1` should be the back projection from 1-D representation `X_pca1`\n",
    "* `X_recon2` should be the back projection from 1-D representation `X_pca2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b7186",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b97e8689a5147020e3c04581fcb9a7c",
     "grade": false,
     "grade_id": "cell-8f6a5252ac9fb291",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_recon1 = None # let this store the 3D reconstruction after first projecting to a 1D PCA space\n",
    "X_recon2 = None # let this store the 3D reconstruction after first projecting to a 2D PCA space\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8868a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e568923c71174475cc150218ca24f14e",
     "grade": true,
     "grade_id": "cell-e751a58466d70975",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_recon1.shape == X.shape)\n",
    "assert(X_recon2.shape == X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed8862",
   "metadata": {},
   "source": [
    "We can reuse our earlier plotting functions to show the reconstructed data as a 3D plot,\n",
    "including the use PCA components.\n",
    "\n",
    "Note that after back projecting, the data is again 3D, but intrinsically low dimensional as all data points now either lie on a 2D plane or 1D line in the 3D feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstruction from 2D PCA')\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca2, X_recon2, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstruction from 1D PCA')\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca1, X_recon1, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147ab3b",
   "metadata": {},
   "source": [
    "### 1.1.6 Supervised dimensionality reduction\n",
    "\n",
    "\n",
    "While PCA is an unsupervised approach, since it doesn't consider the class labels,\n",
    "it is also possible to create a projection that does consider the class labels,\n",
    "for instance to maximize the kept variance between the classes, rather than the variance of the overall data.\n",
    "\n",
    "For intance, we could use LDA to learn such a projection. From Chapter 8:\n",
    "**Linear Discriminant Analysis (LDA)** *is a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane\n",
    "onto which to project the data. The benefit of this approach is that the projection\n",
    "will keep classes as far apart as possible, so LDA is a good technique to reduce\n",
    "dimensionality before running another classification algorithm such as an SVM\n",
    "classifier.*\n",
    "\n",
    "While the book doesn't go into detail on how the use LDA, the [sklearn interface for LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) is very similar to that of PCA for dimensionality reduction.\n",
    "Similar to PCA, you can set `n_components` in the LDA constructor, and perform fit() and transform() operations. Note that since this is supervised LDA, `n_components` can be at most equal to the number of classes - 1. E.g. in a 2 class problem, we can only find a `n_components=1`-dimensional subspace to best separate these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db7668",
   "metadata": {},
   "source": [
    "Use sklearn to create a supervised 1-dimensional LDA projection on the training data $X$, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe567df",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b2a1802460435664bd58ab36e4cde0a",
     "grade": false,
     "grade_id": "cell-f8e377c62b3c812f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_lda1 = None # X_lda1 will be the result of projecting X with LDA to a 1D space \n",
    "\n",
    "# Note that you might need to import the LDA implementation from sklearn first\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21145f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e29068e9160ad11fca2675717e643803",
     "grade": true,
     "grade_id": "cell-c3243dd635560d48",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_lda1.shape == (100,1))\n",
    "assert(X_pca1.shape == (100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlims = (-4, 4)\n",
    "\n",
    "def plot_class_dist_histograms(X_1d, y):\n",
    "    nbins = 10\n",
    "    bins = np.linspace(xlims[0], xlims[1], nbins)\n",
    "    plt.hist(X_1d[y==0], bins, alpha=0.5, label='class 0')\n",
    "    plt.hist(X_1d[y==1], bins, alpha=0.5, label='class 1')\n",
    "    #plt.ylim([0., 1.0])\n",
    "    plt.ylabel('# samples')\n",
    "    plt.xlabel('projected 1D space')\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14,8))    \n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(X_pca1[y==0], np.zeros(X_pca1[y==0].shape), '.')\n",
    "plt.plot(X_pca1[y==1], np.zeros(X_pca1[y==1].shape), '.')\n",
    "plt.title('1D PCA projection')\n",
    "plt.axis('equal')\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(X_lda1[y==0], np.zeros(X_lda1[y==0].shape), '.')\n",
    "plt.plot(X_lda1[y==1], np.zeros(X_lda1[y==1].shape), '.')\n",
    "plt.title('1D LDA project')\n",
    "plt.axis('equal')\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plot_class_dist_histograms(X_pca1, y)\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plot_class_dist_histograms(X_lda1, y)\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a052108",
   "metadata": {},
   "source": [
    "Compare the projected distributions in the plots above for PCA (left column) and LDA (right column). \n",
    "Note that the PCA projection doesn't scale the data to preserve the variance, it just defined a new coordinate system in the original space.\n",
    "The LDA projection on the other hand did scale the data, hence the total variance might appear larger than the PCA variance.\n",
    "\n",
    "**Q** Which dimensionality reduction method will have a lower Bayes error if you would use the projected data in a 1 dimensional classifier afterwards? Motivate your answer by observations that you make in the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95080e9c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de05cc62434ce40ee6b61d93ae3112e6",
     "grade": false,
     "grade_id": "cell-ade4bc3a639c0882",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ANSWER = None\n",
    "# ANSWER = \"PCA\"\n",
    "# ANSWER = \"LDA\"\n",
    "# ANSWER = \"PCA and LDA have a similar Bayes error\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73d982",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a3c9f0882f4cbfaeb178bfa09893927",
     "grade": true,
     "grade_id": "cell-6650da3977c949de",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ANSWER in [\"PCA\",\"LDA\",\"PCA and LDA have a similar Bayes error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a371f14",
   "metadata": {},
   "source": [
    "## 1.2 Implement your own PCA\n",
    "\n",
    "The math behind the standard PCA implementaion is pretty straightforward, and explained in chapter 8 of the book.\n",
    "Let's verify that your implementation performs the same as sklearn's PCA implementation.\n",
    "\n",
    "Complete the following functions in the code blocks below:\n",
    "\n",
    "* `pca_fit(X, n_components)` to estimate the PCA transformation to a `n_component`-dimensional subspace\n",
    "* `pca_transform(m, components, X)` to apply the PCA transformation to data X\n",
    "* `pca_inverse_transform(mean, components, X_pca)` to perform the back-projection to the original feature space\n",
    "\n",
    "These functions will behave similar to sklearn's PCA fit(), transform() and inverse_transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8742379",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6338fc0368cd129ed3c1f615ea3684fb",
     "grade": false,
     "grade_id": "cell-5316bbc7437b8422",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: don't use any sklearn functions in your own (re)-implementation. You can use np.linalg.svd though.\n",
    "\n",
    "def pca_fit(X, n_components):\n",
    "    \"\"\" Given an N x D input data matrix `X` containing N data samples in a D dimensional space,\n",
    "        Compute the parameters of the PCA projection, namely\n",
    "        - mean: the D-dimensional data mean, and\n",
    "        - components: a `n_components` x D matrix containing the first `n_components` PCA components\n",
    "        \n",
    "        Returns: mean, components\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84211422",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c9b78d93ad19022e490094d4918661",
     "grade": true,
     "grade_id": "cell-17597d66bea52c34",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "\n",
    "# run your implementation\n",
    "mean, components = pca_fit(X, n_components=n_comp)\n",
    "\n",
    "assert( mean.shape == (3,) ) # mean should be 3D vector\n",
    "assert( components.shape == (n_comp, 3) ) # each of the n components should be a 3D vector\n",
    "\n",
    "# -- Comparison to Sklearn's PCA implementation --\n",
    "from sklearn.decomposition import PCA\n",
    "pca_ref = PCA(n_components=n_comp)\n",
    "pca_ref.fit(X)\n",
    "\n",
    "# First, we'll check if you have the same mean vector\n",
    "assert(np.all( np.isclose(pca_ref.mean_, mean) ))\n",
    "\n",
    "# Next, we'll check each of the PCA components in turn\n",
    "# Note that since the sign of a component is arbitray, so we should ensure that signs are aligned\n",
    "# before comparing the solutions of different PCA implementations\n",
    "\n",
    "def is_same_within_tolerance(a, b):\n",
    "    # test if two vectors are similar, within a small numerical error\n",
    "    return np.all(np.abs(a - b) < 1e-10)\n",
    "\n",
    "for c in range(n_comp):\n",
    "    ref_comp = pca_ref.components_[c]\n",
    "    your_comp = components[c]\n",
    "    \n",
    "    print(f'testing if components {c} is  similar ...')\n",
    "    print('  sklearn PCA:', ref_comp)\n",
    "    print('     your PCA:', your_comp)\n",
    "    \n",
    "    is_similar = is_same_within_tolerance(ref_comp, your_comp)\n",
    "    is_similar_mirrored = is_same_within_tolerance(ref_comp, -your_comp) # test with flipped sign\n",
    "    \n",
    "    if is_similar:\n",
    "        print('OK: components are the same')\n",
    "    elif is_similar_mirrored:\n",
    "        print('OK: components are the same [only sign is flipped]')\n",
    "    else:\n",
    "        print('ERROR: components are the different!')\n",
    "    print()\n",
    "    assert(is_similar or is_similar_mirrored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6633ea2",
   "metadata": {},
   "source": [
    "Next, implement the transformation function that projects data X to the learned PCA space. Use standard numpy functions only, don't use any sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2711d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "155ad2f55f067e6c950a06bdc84f96c3",
     "grade": false,
     "grade_id": "cell-8001b22339508694",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca_transform(mean, components, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa44c79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a67950c15603d894c83b468373decbfb",
     "grade": true,
     "grade_id": "cell-c9f3db42ae939fc4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_pca = pca_ref.transform(X)\n",
    "X_your_pca = pca_transform(mean, components, X)\n",
    "\n",
    "assert(X_pca.shape == X_your_pca.shape)\n",
    "\n",
    "# Let's create a plot of the resulting projections\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(X_your_pca[:,0], X_your_pca[:,1], '.')\n",
    "plt.title('your implementation')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(X_pca[:,0], X_pca[:,1], 'x')\n",
    "plt.title('sklearn implementation')\n",
    "plt.grid()\n",
    "\n",
    "# again, the signs of the projection might be flipped.\n",
    "#  to ensure that both implementaions use the same sign,\n",
    "#  we'll flip the sign if the sign of the first element is negative\n",
    "X_pca_ = X_pca.copy()\n",
    "X_your_pca_ = X_your_pca.copy()\n",
    "for c in range(n_comp):\n",
    "    if X_pca_[0,c] < 0: X_pca_[:,c] = -X_pca_[:,c]\n",
    "    if X_your_pca_[0,c] < 0: X_your_pca_[:,c] = -X_your_pca_[:,c]\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(X_your_pca_[:,0], X_your_pca_[:,1], '.', label='your implementation')\n",
    "plt.plot(X_pca_[:,0], X_pca_[:,1], 'x', label='sklearn implementation')\n",
    "plt.title('after aligning signs (plots should overlap)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Finally, let's run some asserts to check that the outcomes are indeed the same\n",
    "assert(np.all(np.abs( X_your_pca_ - X_pca_ ) < 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60976d7",
   "metadata": {},
   "source": [
    "Finally, implement the inverse transformation which projects the points from the embedded PCA space back to the original 3D feature space. Use again standard numpy functions only, don't use any sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63502bf3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64aac95e5b339a449fc40b8862eff097",
     "grade": false,
     "grade_id": "cell-1f1f662977a51e04",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca_inverse_transform(mean, components, X_pca):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_reconstruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec56c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cbe605df1871c9f71542670b54b7679",
     "grade": true,
     "grade_id": "cell-24ff5c1c8d44b4d8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your implementation\n",
    "X_your_reconstruct = pca_inverse_transform(mean, components, X_your_pca)\n",
    "\n",
    "# sklearn PCA reference\n",
    "X_ref_reconstruct = pca_ref.inverse_transform(X_pca)\n",
    "\n",
    "# solutions the same?\n",
    "assert( X_ref_reconstruct.shape == X_your_reconstruct.shape )\n",
    "assert( np.all( np.abs( X_ref_reconstruct - X_your_reconstruct ) < 1e-12 ) )\n",
    "\n",
    "# visually verify that your code has projected all points to a 2D plane in the original 3D feature space\n",
    "\n",
    "# Note: using X_your_reconstruct here\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X_your_reconstruct, view_angle1, view_angle2, label_name='feat.'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac7c53",
   "metadata": {},
   "source": [
    "For any given dataset, the number of possible PCA components is bounded by the number of samples and number of features.\n",
    "Express this straightforward relationship by completing the following function.\n",
    "\n",
    "If you don't know the answer, realize that the PCA components are defined by the number of non-zero eigenvalues of the data covariance matrix, e.g. the rank of the covariance matrix. Of course, you could also just experiment with sklearn's implementation and see what works, or look it up somewhere ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70ab89",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bf2b9d5a4fc7c15c2b70b7be8196b7b",
     "grade": false,
     "grade_id": "cell-21e8646bff1e043e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_possible_PCA_components_for_given_dataset(num_samples, num_features):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return max_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52741df9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b32a043b395c5bb1f35f1f4fe351bde",
     "grade": true,
     "grade_id": "cell-c5db4978a2e939f2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test same sample cases\n",
    "num_samples = 3\n",
    "num_features = 3\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 42\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n",
    "\n",
    "num_samples = 300\n",
    "num_features = 1024\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426a480",
   "metadata": {},
   "source": [
    "## 1.3 Eigen-pedestrians\n",
    "\n",
    "Now that we have explored PCA with some toy data, let's apply it to some real world data.\n",
    "You are given a dataset containing 1500 gray-scale pedestrian image patches obtained from a driving vehicle.\n",
    "The $25 \\times 50$ pixel images have been reshaped to 1250-dimensional feature vectors.\n",
    "\n",
    "We start by loading the dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48012961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "data_int = scipy.io.loadmat('data/ped_int.mat')\n",
    "\n",
    "X_ped = data_int['ped_int']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a0148",
   "metadata": {},
   "source": [
    "We first explore the dataset a bit. As we have done in earlier assignments,\n",
    "it is possible to visualize the feature vectors back as  gray-level intensity images by resizing\n",
    "them to their original size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_HEIGHT = 50\n",
    "IM_WIDTH = 25\n",
    "\n",
    "# check we have correcly loaded the data\n",
    "assert(X_ped.shape[1] == IM_HEIGHT * IM_WIDTH)\n",
    "\n",
    "def feat_to_image(X):\n",
    "    return X.reshape(IM_WIDTH, IM_HEIGHT).T\n",
    "\n",
    "def plot_feat_as_image(X):\n",
    "    image = feat_to_image(X)\n",
    "    plt.imshow(image, cmap=mpl.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_multiple_feats_as_images(X, num_cols=10, suptitle=None):\n",
    "    plt.figure(figsize=(16,2))\n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        sample = X[i]\n",
    "        image = feat_to_image(sample)\n",
    "\n",
    "        plt.subplot(1,num_cols,i+1)\n",
    "        plt.imshow(image, cmap=mpl.cm.gray)\n",
    "        plt.title('%d' % i)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle)\n",
    "        \n",
    "print('Number of training samples:', X_ped.shape[0])\n",
    "print('        Number of features:', X_ped.shape[1])\n",
    "\n",
    "plot_multiple_feats_as_images(X_ped, 20, suptitle='First 20 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ee131",
   "metadata": {},
   "source": [
    "### 1.3.1 PCA on image data\n",
    "\n",
    "In all these images there is a pedestrian in the center, with some variations in pose, viewpoint, clothing, lighting conditions, background, body size, etc.\n",
    "We can use PCA to capture the main modes of variation in these images,\n",
    "similar to what we have done in 3D toy example, but now using 1250 features.\n",
    "\n",
    "\n",
    "**Q**: How many PCA components could we maximimally compute for this pedestrian dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7df11f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74ba0be69d38612f48aaf13ce852f8bd",
     "grade": false,
     "grade_id": "cell-6947ec38969cc12a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_PCA_COMPONENTS_FOR_PED_DATASET = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6cda07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15a2bc0ec1c4db98a7f16dd83495aa38",
     "grade": true,
     "grade_id": "cell-cc3eea8b9fc19484",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Maximimum number of PCA components for a pedestrian dataset:')\n",
    "print(MAX_PCA_COMPONENTS_FOR_PED_DATASET)\n",
    "\n",
    "assert(isinstance(MAX_PCA_COMPONENTS_FOR_PED_DATASET, int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f7355",
   "metadata": {},
   "source": [
    "Ok, let's fit a 50-dimensional PCA transformation for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1d6f6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7120841cf7887357d0b4b5510f88512d",
     "grade": false,
     "grade_id": "cell-e0daced1f99829d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca = None # store sklearn's PCA object here\n",
    "X_ped_pca = None # store the PCA projection of X_ped here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ac082",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a5ebb4ab5156b2ddde8b8c9343419bf",
     "grade": true,
     "grade_id": "cell-e5d7e36c3530920e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(not pca is None)\n",
    "assert(X_ped_pca.shape == (1500,50))\n",
    "sklearn.utils.validation.check_is_fitted(pca) # will throw error if not fitted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071df5f",
   "metadata": {},
   "source": [
    "Since the features in X represent the intensity values of the individual pixels in the images,\n",
    "the 1250-dimensional mean feature vector contains the mean intensity value of each pixel over the whole dataset,\n",
    "and can thus also be visualized as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9377d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feat_as_image(pca.mean_)\n",
    "plt.title('Mean feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b526bc",
   "metadata": {},
   "source": [
    "Likewise, the PCA components which represent the main axes of variation in the 1250-dimensional dataset can *also* be visualized as images.\n",
    "\n",
    "We'll refer to these images as the **\"Eigen-pedestrians\"**, similar to the concept of [Eigenfaces](https://en.wikipedia.org/wiki/Eigenface),\n",
    "as we can reconstruct pedestrian images from our target distribution very accuratly using only the mean image and a small number of these eigen-pedestrians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_feats_as_images(pca.components_, 20, suptitle='First 20 PCA components (\"Eigen-pedestrians\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015c831",
   "metadata": {},
   "source": [
    "Note how we can recreate different pedestrian images by taking a linear combinations of these PCA components,\n",
    "and adding them to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show_pca_reconstruction(w0=0, w1=0., w2=0., w3=0., w4=0., w10=0., w18=0.):\n",
    "    # the indices of the PCA components in this function's arguments\n",
    "    idxs = np.array([0, 1, 2, 3, 4, 10, 18])\n",
    "    \n",
    "    # construct weight vector with given values\n",
    "    w = np.array([w0, w1, w2, w3, w4, w10, w18])\n",
    "    \n",
    "    x = pca.mean_ + w.dot(pca.components_[idxs])\n",
    "    \n",
    "    image = feat_to_image(x)\n",
    "    plt.imshow(image, cmap=mpl.cm.gray) # note. mpl.cm.binary inversts the colors\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "wrange = (-1600., 1600.)\n",
    "ipywidgets.interact(plot_show_pca_reconstruction, w0=wrange, w1=wrange, w2=wrange, w3=wrange, w4=wrange, w10=wrange, w18=wrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a11de",
   "metadata": {},
   "source": [
    "## 1.4 Compression\n",
    "\n",
    "We will now try to see how we can compress the pedestrian images by using only a few Eigen-pedestrians.\n",
    "Complete the function below which takes the dataset, and a given number of Eigen-pedestrians (PCA components),\n",
    "and use it to \n",
    "\n",
    "1. first compress the data to `n_components` with PCA, and then.\n",
    "2. reconstruct the images from the compressed representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22927c30",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfbbd532a533cf21f3387e2cb4f1a907",
     "grade": false,
     "grade_id": "cell-874d6fe677771805",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compress_and_decompress_with_pca(X_ped, n_components):\n",
    "    \"\"\" Compress and decompress features in dataset X_ped\n",
    "        by first projecting the data to a PCA subspace with n_components,\n",
    "        and the reconstructing the data from this PCA embedding.\n",
    "        \n",
    "        Input: X_ped - N x M dataset\n",
    "        Input: n_components - integer, target number of components for compression\n",
    "        Returns: X_reconstruct - N x M with reconstructed data after compression\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2b5fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5357285e2f97b4d05e3d678183358d36",
     "grade": true,
     "grade_id": "cell-75f2845e27166e7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components=20)\n",
    "\n",
    "# reconstruction should keep the dimensionality of the input\n",
    "assert(X_reconstruct.shape == X_ped.shape)\n",
    "\n",
    "# after compression with 0 components, all samples should just be equal to the mena\n",
    "#  so we should see no variance left in the data\n",
    "X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components=0)\n",
    "assert(np.all( X_reconstruct.var(axis=0) < 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4aae1f",
   "metadata": {},
   "source": [
    "Now we can below interactivaly visualize the result and see what happens if we increase step by step the number of components from 0 to 1, 2, 3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8107507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_compression(n_components):\n",
    "    X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components)\n",
    "\n",
    "    print(f'num components: {n_components}') \n",
    "    plot_multiple_feats_as_images(X_ped, suptitle='original data')\n",
    "    plot_multiple_feats_as_images(X_reconstruct, suptitle=f'reconstruction with {n_components} PCA components (+ mean)')\n",
    "\n",
    "ipywidgets.interact(plot_pca_compression, n_components=ipywidgets.IntSlider(0, min=0,max=1250, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f739e",
   "metadata": {},
   "source": [
    "You should see that especially in the beginning with few components, adding an additional component has a large impact on the reconstruction quality, since these initial components still explain a lot of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7934c",
   "metadata": {},
   "source": [
    "**Q** If we want to lossy compress a dataset $X$ with $N$ samples and a $M$-dimensional feature space, with only $D$ PCA components, how much storage space have we gained?\n",
    "\n",
    "To compute the compression rate $r$, \n",
    "we have to consider\n",
    "- how many numbers were needed to represent the original data set (i.e. the number of elements in the matrix X), and\n",
    "- how many numbers are needed to represent the compressed data\n",
    "- how many numbers are needed to store the \"compression information\", i.e. the relevant PCA parameters.\n",
    "\n",
    "In fact, you can use the following formula:\n",
    "$r = \\frac{N(X)}{N(X_{pca}) + D*N(comp) + N(mean)}$\n",
    "\n",
    "where:\n",
    "- $N(X)$ is the number of (floating point) numbers in the original data set $X$\n",
    "- $N(X_{pca})$ is the number of numbers in the PCA compressed data set $X_{pca}$\n",
    "- $N(comp)$ is the number of (floating point) numbers to represent a single PCA component\n",
    "- $N(mean)$ is the number of (floating point) numbers to represent the mean feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe07fa3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f73c2bc61b20210ae43a3f4be72caa1",
     "grade": false,
     "grade_id": "cell-6760c8addd6ef802",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_data_compression_rate(N, M, D):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return COMPRESSION_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36515e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88a2f25afba2724653ebc090322298c",
     "grade": true,
     "grade_id": "cell-9e5f8e6c8ffd86c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "N = 1500\n",
    "M = 1024\n",
    "D = 50\n",
    "\n",
    "COMPRESSION_RATIO = compute_data_compression_rate(N, M, D)\n",
    "print(f'With {D} PCA components, {N} samples in a {M}-dimensional feature space')\n",
    "print('can be compressed  by a factor %.2fx' % COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ce663",
   "metadata": {},
   "source": [
    "## 1.5 Explained Variance\n",
    "\n",
    "Let's inspect how the number of PCA components affects the amount of variance in that data that the project keeps.\n",
    "\n",
    "\n",
    "First, let's formalize how we compute the total variance in the data. \n",
    "For some data matrix $X$ of N observations with M features. Let $\\text{Var}_j(X)$ be the variance of feature $j$ over $N$ samples $\\{x_{1j}, ..., x_{Nj} \\}$. Then the total variance in $X$ is calculated by summing the M variances: \n",
    "\n",
    "$$ \\text{total\\_variance}(X) = \\sum_{1 \\leq j \\leq M} \\text{Var}_j(X)$$\n",
    "\n",
    "\n",
    "Implement the function which computes the total variance in a given $N \\times M$ data matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf5751",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc7f23647b0991c718db65c8141c2622",
     "grade": false,
     "grade_id": "cell-4a6afdd103fd2524",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def total_variance_in_data(X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e97fae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6567bdb9e4c71d05a3693be9835be125",
     "grade": true,
     "grade_id": "cell-4a592505b4d2b6f2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_toy = np.array([[0,1], [10,2], [20,3]])\n",
    "# total variance on X_toy is (variance([0,10,20]) + variance([1,2,3]))\n",
    "assert(np.abs( total_variance_in_data(X_toy) - (np.var([0,10,20])+np.var([1,2,3])) ) < 1e-6)\n",
    "\n",
    "total_variance_in_X_ped = total_variance_in_data(X_ped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019dab6",
   "metadata": {},
   "source": [
    "With this definition, we can easily compute the amount of variance from the original data `X` that is maintained in in `X_pca`, the data after the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafedf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variance_ratio_in_data(X, X_pca):\n",
    "    \"\"\" Compute the ratio of variance in the original data X\n",
    "        still present in compressed data X_pca:\n",
    "        \n",
    "           total-ratio = amount-variance-in-X_pca / amount-variance-in-X\n",
    "        \n",
    "        Note that the amount of variance of a dataset X is \n",
    "        defined as the sum of the variance of each feature in that data.\n",
    "    \"\"\"\n",
    "    var_in_X = total_variance_in_data(X)\n",
    "    var_in_X_pca = total_variance_in_data(X_pca)\n",
    "    total_variance_ratio = var_in_X_pca / var_in_X\n",
    "    \n",
    "    return total_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869e3b0",
   "metadata": {},
   "source": [
    "Calculate the total variance explained by the PCA components relative to the total variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d83ee4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "146e9d13588066bc901719fb42cd0ab2",
     "grade": true,
     "grade_id": "cell-a0f009cecf41f588",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "variance_explained_ratio = total_variance_ratio_in_data(X_ped, X_ped_pca)\n",
    "print('Ratio of kept variance by comparing X_ped and X_ped_pca: %.2f' % (variance_explained_ratio*100.))\n",
    "\n",
    "# these numbers should be identical\n",
    "assert 0.<= variance_explained_ratio and variance_explained_ratio <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf4ada",
   "metadata": {},
   "source": [
    "**Q**: What is the minimum number of PCA components if we want to keep at least 80% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d1c81",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a224192a71e09b5be1c58776cccf7cd7",
     "grade": false,
     "grade_id": "cell-050f80b6fc1038de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_COMPONENTS_NEEDED = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687d3cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bfe05da50d82a7d4983cb6fcd6f40ab",
     "grade": true,
     "grade_id": "cell-6c923deab5862fab",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer: at least', NUMBER_OF_COMPONENTS_NEEDED, 'are needed to explain 80% of the variance in the data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e85bb",
   "metadata": {},
   "source": [
    "# 2. Clustering\n",
    "\n",
    "We start again by exploring some clustering techinques on a toy dataset,\n",
    "which is generated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create toy dataset\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples,\n",
    "    cluster_std=[0.5, 2.0, 1.5],\n",
    "    random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plot of the data\n",
    "def plot_data_with_labels(X, y=None):\n",
    "    s = plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    if not y is None:\n",
    "        plt.legend(*s.legend_elements())\n",
    "        \n",
    "plot_data_with_labels(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4910574",
   "metadata": {},
   "source": [
    "One common algorithm used to cluster data is **k-means**.\n",
    "In the block below,\n",
    "\n",
    "* use sklearn's k-Means implementation to cluster the given data `X`, and predict each sample's cluster label. Initialize k-Means by assuming that there are $k=3$ clusters in the data.\n",
    "* also obtain the predictions of the fitted k-Means solutions on all samples in `X`, and call this `y_pred_km`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9eca48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7e64405ee88fa25c9690623ea15f323",
     "grade": false,
     "grade_id": "cell-c858924165ff1e40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "kmeans = None # store sklearns k-Means implementation here\n",
    "y_pred_km = None # store the predicted cluster labels for data X here \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a35cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa011e2c576bdb42e3e28e002edd4b7",
     "grade": true,
     "grade_id": "cell-b1698a40756eb52e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert( kmeans != None )\n",
    "assert( len(np.unique(y_pred_km)) == 3 )\n",
    "assert( y_pred_km.shape == (n_samples,) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b84329",
   "metadata": {},
   "source": [
    "Let's checkout the cluster labels that the algorithm found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the samples with the predicted labels\n",
    "plot_data_with_labels(X, y_pred_km)\n",
    "\n",
    "# also show the cluster centers found by k-Means\n",
    "plt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77033ea0",
   "metadata": {},
   "source": [
    "Alternatively, we could fit a Gaussian Mixture distribution on the data,\n",
    "and label the samples based on their probability under the fitted mixture components.\n",
    "\n",
    "In the code below, fit a Gaussian Mixture on the data using $k=3$ mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478d757",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8963ce359d385e98cfa3dadabe1ede25",
     "grade": false,
     "grade_id": "cell-3c71cdcaf943a55c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gm = None # store your sklearn GaussianMixture here\n",
    "y_pred_gm = None # store the predicted class labels from the GaussianMixture here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9038bdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ab4e7a2e7a3f3398749dfbe80eca3b0",
     "grade": true,
     "grade_id": "cell-e1e3b25f849405be",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert( gm != None )\n",
    "assert( len(np.unique(y_pred_gm)) == 3 )\n",
    "assert( y_pred_gm.shape == (n_samples,) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a44f58",
   "metadata": {},
   "source": [
    "Let's again visualize the result, and compare it to k-Means.\n",
    "You should see that the plots look pretty similar, except perhaps for a few samples near the cluster boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c52956",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'rd')\n",
    "plot_data_with_labels(X, y_pred_km)\n",
    "plt.title('k-Means')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(gm.means_[:,0], gm.means_[:,1], 'rd')\n",
    "plot_data_with_labels(X, y_pred_gm)\n",
    "plt.title('Gaussian Mixture')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9567812",
   "metadata": {},
   "source": [
    "## 2.1 Implement k-means algorithm\n",
    "\n",
    "The basic k-Means algorithm actually consists only a few steps:\n",
    "\n",
    "1. initialize k cluster centers by picking k random samples\n",
    "2. assign samples to nearest cluster center\n",
    "3. for each cluster k, compute the new center as the mean of the assigned samples\n",
    "4. go to step 2, until convergence or sufficient iterations have passed\n",
    "\n",
    "In the code blocks below, you will implement steps 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641763fb",
   "metadata": {},
   "source": [
    "Implement **step 1**: select k samples as random samples from X and set these as cluster centers.\n",
    "*HINT:* you can use `np.random.choice` to randomly select k indices from a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fdc14",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28d75fae7f21ef60a296b2ac560509aa",
     "grade": false,
     "grade_id": "cell-b8cd2167361538be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_centers(k, X):\n",
    "    \"\"\" randomly select k rows from data matrix X\n",
    "        and return these as the k cluster centers.\n",
    "        \n",
    "        input: k - an integer\n",
    "        input: X - a N x D matrix\n",
    "        output: center - a k x D matrix containing k initial cluster centers\n",
    "    \"\"\" \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83edc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b809a9df06665a3dfecc1d6801c577df",
     "grade": true,
     "grade_id": "cell-15ba7d96be57b058",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "np.random.seed(5)\n",
    "centers = initialize_centers(k, X)\n",
    "\n",
    "assert(centers.shape == (k, 2)) # k centers in a 2D feature space\n",
    "\n",
    "centers_init = centers.copy() # keep a copy to compare later to\n",
    "\n",
    "# let's plot the randomly selected cluster centers\n",
    "plot_data_with_labels(X)\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plt.title('Initial cluster centers (no data assigned yet)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e9dd9",
   "metadata": {},
   "source": [
    "Implement **step 2**: assign all data points to nearest centers (in terms of Euclidean distance)\n",
    "*Hint:* you can use `scipy.spatial.distance.cdist` to quickly compute the distance of the samples to the cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4403b0f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04891792a18a60c378b035a1ec00a78c",
     "grade": false,
     "grade_id": "cell-08c553fc7f5d3ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def assign_samples_to_centers(centers, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return cluster_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736aa992",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f9ad847e740ed802942b22ee9712bdd",
     "grade": true,
     "grade_id": "cell-167bfcea73922c65",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_idxs = assign_samples_to_centers(centers, X)\n",
    "\n",
    "assert( cluster_idxs.shape == (n_samples,) )\n",
    "assert(cluster_idxs.min() >= 0) # lowest possible cluster id is 0\n",
    "assert(cluster_idxs.max() < k) # highest possible cluster id is k-1\n",
    "\n",
    "# Let's see how the samples were assigned to the randomly picked cluster centers\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X, cluster_idxs)\n",
    "plt.title('Assigning data to nearest cluster centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6fe95",
   "metadata": {},
   "source": [
    "Implement **step 3**: compute new center means based on the assigned cluster labels.\n",
    "In this step, iterate over the k cluster labels, and compute the center of cluster $c$ as the mean of all samples in X assigned to $c$ in `cluster_idxs`.\n",
    "The result is a new set of $k$ cluster centers, based on the current assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910c5fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d842c435420eb2f7677f10a0a2ed4c8",
     "grade": false,
     "grade_id": "cell-2595920badf8056e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_center_means(k, X, cluster_idxs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839ec41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f6b90d7fdd71a55338800745cdf82d7",
     "grade": true,
     "grade_id": "cell-84d75b6d06f813a9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "centers = update_center_means(k, X, cluster_idxs)\n",
    "\n",
    "assert(centers.shape == (k, 2)) # k centers in a 2D feature space\n",
    "\n",
    "# check that the centers have changed w.r.t. the initial centers that we stored in center_init above\n",
    "assert(np.all( centers_init != centers ))\n",
    "\n",
    "# Show the new assignment\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X, cluster_idxs)\n",
    "plt.title('After recomputing the cluster centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0cf76",
   "metadata": {},
   "source": [
    "That's it! If you have implemented these steps correctly, we can run all step sequentially for a few iterations.\n",
    "Notice how after a few iterations already the cluster centers are not moving much anymore.\n",
    "Once we detect that the number of samples assigned to each cluster haven't changed, we can quit iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3857a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "# Step 1\n",
    "centers = initialize_centers(k, X)\n",
    "\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X)\n",
    "plt.title('initialization')\n",
    "plt.show()\n",
    "\n",
    "max_iters = 10\n",
    "last_cluster_counts = np.zeros(k)\n",
    "for j in range(max_iters):\n",
    "    # Step 2\n",
    "    cluster_idxs = assign_samples_to_centers(centers, X)\n",
    "    \n",
    "    # Step 3\n",
    "    centers = update_center_means(k, X, cluster_idxs)\n",
    "\n",
    "    # Show intermediate result\n",
    "    cluster_counts = np.bincount(cluster_idxs, minlength=k)\n",
    "    plt.title(f'iter {j}: ' + str(cluster_counts))\n",
    "    plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "    plot_data_with_labels(X, cluster_idxs)\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 4: continue with next iteration, unless converged\n",
    "    if np.all(cluster_counts == last_cluster_counts):\n",
    "        # detected cluster assignment didn't change anymore since last iteration,\n",
    "        # so cluster centers have converged now\n",
    "        print(f'Converged in {j} iterations!')\n",
    "        break # quit the for-loop\n",
    "    else:\n",
    "        # keep this iteration's cluster counters to compare to in next iteration\n",
    "        last_cluster_counts = cluster_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa08a3",
   "metadata": {},
   "source": [
    "### 2.1.1 Finding the outliers\n",
    "\n",
    "Let's try to do some anomaly detection, that is finding outliers which are dissimilar to most of the rest of the taining data. One approach one might come up with is to consider any sample \"sufficiently\" far away from a cluster center as an outlier. With k-Means we can reuse the scoring metric used to assign samples to cluster centers, thus a sample is considered an outlier if its Euclidean distance to nearest cluster center is above a certain distance threshold.\n",
    "\n",
    "Complete the function `find_kmeans_distance_outliers()` below to implement this outlier detection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eef05d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d5551b3c4919d32f544a8dcb73a36ac",
     "grade": false,
     "grade_id": "cell-21916fcd16d7f444",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_kmeans_distance_outliers(kmeans, X, dist_thresh):\n",
    "    \"\"\"\n",
    "    Determine for each sample in X if it is an outlier or not.\n",
    "    Outliers are found by finding the distance to the closest cluster center.\n",
    "    If this smallest distance is above the given threshold `dist_thresh`,\n",
    "    then the sample is an outlier\n",
    "    \n",
    "    Input: kmeans - a fitted instance of sklearn's KMeans class\n",
    "    Input: X - a N x M dataset of N samples with M features\n",
    "    Input: dist_thresh - a number, outliers are more distance than this to all cluster centers\n",
    "    Returns: is_outlier - a N-dimensional Boolean numpy vector,\n",
    "             is_outlier[i] is True only if sample X[i] is an outlier.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb1e67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa94a7b00eeea724a6103753cb4217a5",
     "grade": true,
     "grade_id": "cell-3b424ecaeb1ad495",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "is_outlier = find_kmeans_distance_outliers(kmeans, X, dist_thresh=2.0)\n",
    "assert(len(is_outlier) == n_samples)\n",
    "assert(is_outlier.dtype == bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7693c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widget to explore outliers\n",
    "def show_kmeans_outliers(dist_thresh):\n",
    "    is_outlier = find_kmeans_distance_outliers(kmeans, X, dist_thresh)\n",
    "    \n",
    "    plt.plot(X[~is_outlier,0], X[~is_outlier,1], '.', label='inlier')\n",
    "    plt.plot(X[is_outlier,0], X[is_outlier,1], '.', label='outlier')\n",
    "    plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "\n",
    "ipywidgets.interact(show_kmeans_outliers, dist_thresh=(0., 5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cd22c",
   "metadata": {},
   "source": [
    "However, a better way could be to use a density estimation technique, such as a Gaussian Mixture Model, as seen in the book. To use the Gaussian Mixture for outlier detection, we could now score the samples with respect to the distribution fit on all the data,  i.e. we compute the log probability density of each sample under the distribution.\n",
    "An outlier is than a sample for which the log probability is under a given threshold\n",
    "\n",
    "As you did for k-Means outlier detection, complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55097a62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7e3127f6d78afcc5a66c6ac19d9b6dc",
     "grade": false,
     "grade_id": "cell-c90fa1f7675f118f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_gm_logprob_outliers(gm, X, logprob_thresh):\n",
    "    \"\"\"\n",
    "    Determine for each sample in X if it is an outlier or not.\n",
    "    Outliers are found by finding the log-probability under the fitted Gaussian mixture.\n",
    "    If this log probability is BELOW the given threshold `logprob_thresh`,\n",
    "    then the sample is an outlier.\n",
    "    \n",
    "    Input: gm - a fitted instance of sklearn's GaussianMixture class\n",
    "    Input: X - a N x M dataset of N samples with M features\n",
    "    Input: logprob_thresh - a number, outliers how a log-probability under the GM below this threshold\n",
    "    Returns: is_outlier - a N-dimensional Boolean numpy vector,\n",
    "             is_outlier[i] is True only if sample X[i] is an outlier.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5066c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ff05b82973e924ce4571d5461f60cec",
     "grade": true,
     "grade_id": "cell-ba29f37e6837d6f7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "is_outlier = find_gm_logprob_outliers(gm, X, logprob_thresh=-7.0)\n",
    "assert(len(is_outlier) == n_samples)\n",
    "assert(is_outlier.dtype == bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88967a9",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of log probabilities for our dataset, and see if we can get an idea in what range we should put our cutoff threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gm.score_samples(X));\n",
    "plt.xlabel('log probability')\n",
    "plt.ylabel('number of samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe31ec4",
   "metadata": {},
   "source": [
    "We can see that most samples have a log probability above approximately -7.\n",
    "A cutoff threshold in the range -8 and -6 is probably good, we could reasonably argue that the samples in the tail of this distribution are outliers. Of course in practice the cutoff would be depend on your task, so this is not a general rule.\n",
    "\n",
    "We can study the effect of different thresholds in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widget to explore outliers\n",
    "def show_gm_outliers(logprob_thresh):\n",
    "    is_outlier = find_gm_logprob_outliers(gm, X, logprob_thresh)\n",
    "    \n",
    "    plt.plot(X[~is_outlier,0], X[~is_outlier,1], '.', label='inlier')\n",
    "    plt.plot(X[is_outlier,0], X[is_outlier,1], '.', label='outlier')\n",
    "    plt.plot(gm.means_[:,0], gm.means_[:,1], 'rd')\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "\n",
    "ipywidgets.interact(show_gm_outliers, logprob_thresh=(-12., .0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976be30",
   "metadata": {},
   "source": [
    "Compare the earlier outlier detection method using the Euclidean distance to the cluster center to this approach of using the log probability under the GMM.\n",
    "\n",
    "**Q**: What happens when you try to detect possible outliers in the smaller cluster in the bottom-right? Explain what you observe with the Euclidean-distance based detection method, and what you observe with the Gaussian Mixture detection method. What causes these differences?\n",
    "\n",
    "*Hint*: think about how the distance between the a sample and the center (mean) are used in the formula for the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2e918",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a6485d35373308f993f2fbf12a439b",
     "grade": true,
     "grade_id": "cell-42c2c9f88f758989",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e723b16",
   "metadata": {},
   "source": [
    "We end this part with a note.\n",
    "\n",
    "An even better way to find outliers with respect to the overall data, would be to fit the distribution on all data EXCEPT the sample we are scoring, since we are now 'testing' the distribution on a sample in the 'training' data.\n",
    "This is what the book refers to *novelty detection*, but an outlier could be consider a novelty w.r.t. the rest of the training data.\n",
    "\n",
    "However, since in these examples we are using few mixture components the overall distributions should not change too much of we keep the test sample included (bias-variance trade-off) and it is much faster to fit the distribution once and test all N samples, instead of fitting a distribution N times for all test samples separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30bc7c",
   "metadata": {},
   "source": [
    "## 2.2 Clustering faces\n",
    "\n",
    "For the final part, let's consider a social robot that moves around a building,\n",
    "and continously bumps into people. Using a trained object detector,\n",
    "it can detect and locate faces in its camera image.\n",
    "Using the bounding box, a square patch around a detected face could be extracted.\n",
    "Over time, the robot would build a dataset of faces, some belonging to the same people and others not.\n",
    "Can you help the robot determine which faces probably correspond to the same person?\n",
    "\n",
    "![Pepper, the social robot (image source: Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/SoftBank_pepper.JPG/330px-SoftBank_pepper.JPG)\n",
    "\n",
    "\n",
    "For this exercise we will use the `Olivetti faces dataset`, containing 400 data samples, with 10 samples for 40 persons.\n",
    "Each sample is a $64 \\times 64$ gray scale image patch, reshaped into a 4096-dimensional feature vector.\n",
    "The class labels of this dataset represent the person identitiy, so all samples with the same class label are taken from the same person.\n",
    "We will pretend that these are the (gray scale) face image patches that our robot has collected.\n",
    "The dataset actually contains a set of face images taken between April 1992 and \n",
    "April 1994 at AT&T Laboratories Cambridge. \n",
    "\n",
    "\n",
    "Note that the first time you run the code below, sklearn will automatically downloads the data\n",
    "archive from AT&T. The result will be cached, so once it is on your computer it will not need to download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "ALLOW_DOWNLOAD = True\n",
    "\n",
    "#data = sklearn.datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4, download_if_missing=ALLOW_DOWNLOAD)\n",
    "data = sklearn.datasets.fetch_olivetti_faces(download_if_missing=ALLOW_DOWNLOAD)\n",
    "\n",
    "# To see more information on the downloaded dataset, execute the following line:\n",
    "#print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['data'], data['target']\n",
    "\n",
    "print('Dataset size:', X.shape)\n",
    "print('Unique person ids:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88739aa0",
   "metadata": {},
   "source": [
    "To get an idea of the contents of this dataset, we can explore the features in this dataset by resizing them to $64 \\times 64$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_face(idx):\n",
    "    x = X[idx]\n",
    "    x = x.reshape((64,64))\n",
    "    label = y[idx]\n",
    "    plt.imshow(x, cmap='gray')\n",
    "    plt.title(f'sample={idx}, label={label}')\n",
    "    plt.axis('off')\n",
    "\n",
    "ipywidgets.interact(show_face, idx=(0,399))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fbeed",
   "metadata": {},
   "source": [
    "In the code block below, compare 3 methods of dimensionality reduction to visualize the data in the 4096-dimensional space in a 2D plot, namely:\n",
    "\n",
    "1. Principal Component Analysis (see Part 1 of the exercises)\n",
    "2. Linear Discriminant Analysis (see Part 1 of the exercises)\n",
    "3. t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Check the sklearn documentation on [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE) if you are not sure how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8048a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09715c7dcac84f7e541882592f48b4ee",
     "grade": false,
     "grade_id": "cell-d14a6c7642c6e8b5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set these variables\n",
    "X_pca = None # result of dim. reduction with PCA\n",
    "X_lda = None # result of dim. reduction with LDA\n",
    "X_tsne = None # result of dim. reduction with t-SNE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e77c64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b53ee72737eebaab1d98d1d8dc18abe",
     "grade": true,
     "grade_id": "cell-5b4fbed90cd8d36e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca.shape == (400, 2))\n",
    "assert(X_lda.shape == (400, 2))\n",
    "assert(X_tsne.shape == (400, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb189a",
   "metadata": {},
   "source": [
    "Afterwards, we can visualize the data in the projected space.\n",
    "We will also in color the points in all plots using the true class labels, such that we can see if the resulting embedding has placed points from the same class together or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed937820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_embedded_classes(X, y):\n",
    "    for c in np.unique(y):\n",
    "        plt.plot(X[y==c,0], X[y==c,1], '.', label='%d'%c)\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plot_2D_embedded_classes(X_pca, y)\n",
    "plt.title('PCA')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_2D_embedded_classes(X_lda, y)\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_2D_embedded_classes(X_tsne, y)\n",
    "plt.title('TSNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3708060",
   "metadata": {},
   "source": [
    "**Q1:** Which of these 3 dimensionality reduction techniques appear deterministic, and which stochastic (i.e. provide different results every time you run it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd988c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c0710db5ebd0139cf4315feb7f1ecc4",
     "grade": false,
     "grade_id": "cell-86f3b8202628552a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To answer, set each variable to either True or False\n",
    "\n",
    "#Q1\n",
    "PCA_IS_DETERMINISTIC = None\n",
    "LDA_IS_DETERMINISTIC = None\n",
    "TSNE_IS_DETERMINISTIC = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cfcdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "355473428bf3ca76fccaa91286ab4215",
     "grade": true,
     "grade_id": "cell-1a78e8be16ec96a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(PCA_IS_DETERMINISTIC in (True, False))\n",
    "assert(LDA_IS_DETERMINISTIC in (True, False))\n",
    "assert(TSNE_IS_DETERMINISTIC in (True, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff60f29",
   "metadata": {},
   "source": [
    "**Q2:** Which of these 3 dimensionality reduction techniques are unsupervised, and thus 'blind' to the true class labels (until) we plot them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76665c6d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053d81575ca35f55b745ab596be58840",
     "grade": false,
     "grade_id": "cell-f634aa0ebefac90d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To answer, set each variable to either True or False\n",
    "\n",
    "#Q2\n",
    "PCA_IS_UNSUPERVISED = None\n",
    "LDA_IS_UNSUPERVISED = None\n",
    "TSNE_IS_UNSUPERVISED = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a8eff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fcfe9fb5bb937873fd510d5478f0184",
     "grade": true,
     "grade_id": "cell-f843e605d0ec21da",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(PCA_IS_UNSUPERVISED in (True, False))\n",
    "assert(LDA_IS_UNSUPERVISED in (True, False))\n",
    "assert(TSNE_IS_UNSUPERVISED in (True, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1856f",
   "metadata": {},
   "source": [
    "**Q** Which of these 3 dimensionality reduction techniques compute non-linear projections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651fedf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0136fbfa926ced004f37065df6e7638d",
     "grade": false,
     "grade_id": "cell-d68b2299995068a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To answer, set each variable to either True or False\n",
    "\n",
    "#Q3\n",
    "PCA_IS_NONLINEAR_PROJECTION = None\n",
    "LDA_IS_NONLINEAR_PROJECTION = None\n",
    "TSNE_IS_NONLINEAR_PROJECTION = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759fa57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91780fa29f6bf614d3bb7e18c8e51aad",
     "grade": true,
     "grade_id": "cell-89f6aaf615f8e477",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(PCA_IS_NONLINEAR_PROJECTION in (True, False))\n",
    "assert(LDA_IS_NONLINEAR_PROJECTION in (True, False))\n",
    "assert(TSNE_IS_NONLINEAR_PROJECTION in (True, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4fae8a",
   "metadata": {},
   "source": [
    "### 2.2.1 Comparing k-Means and Gaussian Mixture model\n",
    "\n",
    "Ok, now let's see what happens if we try to recover the 40 identities by clustering this data.\n",
    "\n",
    "We do need a metric to evaluate success though.\n",
    "There are many metrics in sklearn to evaluate the cluster quality with respect to some ground truth labels.\n",
    "Here, we will use `sklearn.metrics.completeness_score()`.\n",
    "\n",
    "From the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score):\n",
    "```\n",
    "Completeness metric of a cluster labeling given a ground truth.\n",
    "\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "```\n",
    "\n",
    "Overall, this metric will be scored between 0.0 for bad (all the clusters are randomly divided over the true class labels) to 1.0 for good (clusters perfectly align with class labels, up to permutation of which class is which cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a72df",
   "metadata": {},
   "source": [
    "Next, try to cluster the data into 40 clusters using\n",
    "\n",
    "- k-Means, and\n",
    "- a Gaussian Mixture model.\n",
    "\n",
    "**WARNING** Gaussian component has a $M \\times M$ covariance matrix, which results in a large amount of parameters to be estimated as the number of feature increases (it grows quadratically!).\n",
    "Fitting a mixture model on such high dimensional feature space can therefore take a *loooooong* time before it converges, and will also be very prone to overfitting!\n",
    "\n",
    "**Hint:** First apply PCA to reduce the feature space to, say, 50 components before fitting the Gaussian Mixture.\n",
    "\n",
    "Remember that to debug your code, and you can use `%%time` to figure out how long it takes to run a block on your PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60713e5b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b84c68009de5a598794aecf6a36e17",
     "grade": false,
     "grade_id": "cell-1cb6992161033640",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans = None # store your KMeans instance in this variable\n",
    "y_pred_km = None # store your predictions in this variable\n",
    "\n",
    "k = 40\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b9719",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e198753ca9784cc34b664bc921e739",
     "grade": false,
     "grade_id": "cell-d35933b9a3e81c1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gm = None # store your GaussianMixture in this variable\n",
    "y_pred_gm = None # store your predictions in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c7b9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec2208c57b69eeec208fa4cd5fcdd15",
     "grade": true,
     "grade_id": "cell-62cea27bfac0cd92",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "score_km = sklearn.metrics.completeness_score(y, y_pred_km)\n",
    "print('         k-Means:', score_km)\n",
    "\n",
    "score_gm = sklearn.metrics.completeness_score(y, y_pred_gm)\n",
    "print('Gaussian Mixture:', score_gm)\n",
    "\n",
    "assert( kmeans != None )\n",
    "assert( len(np.unique(y_pred_km)) == 40 )\n",
    "assert( y_pred_km.shape == (400,) )\n",
    "\n",
    "assert( gm != None )\n",
    "assert( len(np.unique(y_pred_gm)) == 40 )\n",
    "assert( y_pred_gm.shape == (400,) )\n",
    "\n",
    "sklearn.utils.validation.check_is_fitted(kmeans) # will throw error if not fitted\n",
    "sklearn.utils.validation.check_is_fitted(gm) # will throw error if not fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c8c66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03ec8c0a56adcb1519170d3092870e43",
     "grade": false,
     "grade_id": "cell-05d8795c647887b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that you might see some variance in the results everytime you rerun it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65021206",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf5a1d4973adac4b1cbb12545c4e6fa8",
     "grade": false,
     "grade_id": "cell-618445b3aa549c6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q**: Can you conclude which clustering method finds better clusters on this dataset? (better performance means one method can outperform the other by at least 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99823244",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50bcb8748e04044a347a3703ab28ddc9",
     "grade": false,
     "grade_id": "cell-e829d49b9b6206a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ANSWER = None\n",
    "# ANSWER = \"k-Means\"\n",
    "# ANSWER = \"Gaussian Mixture\"\n",
    "# ANSWER = \"We cannot conclude that one outperforms the other.\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42668a19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa3835f683646ca2b7e7db5bcffc8882",
     "grade": true,
     "grade_id": "cell-98dcdb9661d5ad7a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ANSWER in [\"k-Means\",\"Gaussian Mixture\",\"We cannot conclude that one outperforms the other.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52336854",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "622ffba72e92706f8f9537564a16b2fa",
     "grade": false,
     "grade_id": "cell-b3d7536b96458bb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q**: Which approach clusters the data more efficiently in terms of processing time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b07340",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dea99f4e9d6d8cf3b4469121ed764bb",
     "grade": false,
     "grade_id": "cell-856cc8fa825cf514",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ANSWER = None\n",
    "# ANSWER = \"k-Means without dimensionality reduction is more efficient\"\n",
    "# ANSWER = \"Gaussian Mixture with dimensionality reduction is more efficient\"\n",
    "# ANSWER = \"We cannot conclude that one is faster than the other, as they have very similar processing time\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f3fe9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c15064357af1127d9f6bdfe7dd0506",
     "grade": true,
     "grade_id": "cell-37341bc18f467a5e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ANSWER in [\n",
    "    \"k-Means without dimensionality reduction is more efficient\",\n",
    "    \"Gaussian Mixture with dimensionality reduction is more efficient\",\n",
    "    \"We cannot conclude that one is faster than the other, as they have very similar processing time\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7609e",
   "metadata": {},
   "source": [
    "For fun, let's visually inspect the clusters found by both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e78997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces_class(y_pred, c):\n",
    "    idxs = np.where(y_pred == c)[0]\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for j, idx in enumerate(idxs[:15]):\n",
    "        plt.subplot(3,5,j+1)\n",
    "        show_face(idx)\n",
    "\n",
    "print('K-Means')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_km, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gaussian Mixture')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_gm, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94bafa",
   "metadata": {},
   "source": [
    "## 2.3 Semi-supervised learning\n",
    "\n",
    "For the final test, let's assume we DO have a few labels, but not all labels.\n",
    "In fact, let's assume that our robot gets one face with a person id for each person,\n",
    "but also collects 9 unlablled pictures of each person.\n",
    "Can we improve our clustering by using the few labelled samples?\n",
    "\n",
    "This would be a case of *semi-supervised* learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fe78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that this part of the data is labelled\n",
    "# every 10th sample is a new person\n",
    "X_labelled = X[::10,:]\n",
    "y_labelled = y[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba977e8b",
   "metadata": {},
   "source": [
    "As a baseline, let's see how a standard Random Forest classifier would perform when given just this 1-sample per class dataset on the full unlabelled dataset.\n",
    "\n",
    "**NOTE**: a better way to draw sound conclusions would be to do a proper train-and-test split and hyperparameter tuning, but a Random Forest is a relatively robust classifier with few hyperparameters, so it is sufficient for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b92843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "rf = sklearn.ensemble.RandomForestClassifier()\n",
    "rf.fit(X_labelled, y_labelled)\n",
    "y_pred_rf = rf.predict(X)\n",
    "\n",
    "comp_score = sklearn.metrics.completeness_score(y, y_pred_rf)\n",
    "acc_score = sklearn.metrics.accuracy_score(y, y_pred_rf)\n",
    "\n",
    "print('RF completeness:', comp_score)\n",
    "print('    RF accuracy:', acc_score)\n",
    "\n",
    "print('Confusion matrix')\n",
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_rf, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8922288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forst (supervised on 10 samples)')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_rf, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307441b8",
   "metadata": {},
   "source": [
    "The supervised accuracy of the RF is not great ... but it also was given very little data (1 sample per class!)\n",
    "\n",
    "Let's try to use a GaussianMixture here again, but let's give it a semi-supervised twist:\n",
    "**use the given labelled samples as initial mean estimates of the 40 Gaussian terms**.\n",
    "\n",
    "To do this, take care of the following points:\n",
    "\n",
    "- You will need to use PCA for dimensionality reduction again, try using about 18 components this time\n",
    "- PCA should be fitted on all data, but you need to project the labelled data to the PCA space again separately to define the means of the GaussianMixture in this reduced space\n",
    "- To initialize the means of the GaussianMixture with specific values, you need to set the `init_params` parametere to `'random'`, and then use the `means_init` parameter to define the means. See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9406b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb0e9c04f795e5126e3686a311f4569a",
     "grade": false,
     "grade_id": "cell-9d42c608723ea944",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gm2 = None # store your semi-supervised GaussianMixture in this variable\n",
    "y_pred_gm2 = None # store the predictions of the semi-supervised GaussianMixture in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9b664",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81d3277d77f093a9acc5b20413850289",
     "grade": true,
     "grade_id": "cell-2df81c72fddd435f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "comp_score_gm2 = sklearn.metrics.completeness_score(y, y_pred_gm2)\n",
    "acc_score_gm2 = sklearn.metrics.accuracy_score(y, y_pred_gm2)\n",
    "\n",
    "print('GM semi-supervised completeness:', comp_score_gm2)\n",
    "print('   GM semi-supervised  accuracy:', acc_score_gm2)\n",
    "\n",
    "# these scores should be achievable (though there is a bit of randomness ...)\n",
    "assert(comp_score_gm2 >= 0.82)\n",
    "assert(acc_score_gm2 >= 0.75)\n",
    "\n",
    "assert( gm2 != None )\n",
    "assert( len(np.unique(y_pred_gm2)) == 40 )\n",
    "assert( y_pred_gm2.shape == (400,) )\n",
    "\n",
    "sklearn.utils.validation.check_is_fitted(gm2) # will throw error if not fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f18fd",
   "metadata": {},
   "source": [
    "If everything is correct, you should see that semi-supervised training with the GaussianMixture outperforms the fully unsupervised Gaussian Mixture, but also the fully supervised Random Forest!\n",
    "\n",
    "\n",
    "Another benefit is that the cluster labels now all align with the true class labels, because of the small amount of supervision, as the confusion matrix below will confirm (it should only be diagonal if the cluster labels are equal to the true class labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_gm2, y))\n",
    "plt.title('GM (semi-supervised)')\n",
    "\n",
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_rf, y))\n",
    "plt.title('RF (supervised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Semi-supervised Gaussian Mixture')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_gm2, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af7144",
   "metadata": {},
   "source": [
    "This concludes this week's exercises!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
